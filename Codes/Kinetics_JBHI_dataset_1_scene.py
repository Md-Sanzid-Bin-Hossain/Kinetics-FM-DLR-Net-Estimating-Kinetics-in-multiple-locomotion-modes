# -*- coding: utf-8 -*-
"""Future_Gait_DLR_Net_public_data_Stair_slope_Kinematics_kinetics_Public_dataset_kinematics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/116DLyfghavEPPeefrKabrJvohcuH2_xs
"""

# Let's import all packages that we may need:
import numpy
import tensorflow as tf
import statistics 
from numpy import loadtxt
import matplotlib.pyplot as plt
import pandas
import math
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import GRU,LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statistics import stdev 
import math

 
import numpy as np

from scipy.signal import butter,filtfilt
 
import sys 
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
import pandas
import matplotlib.pyplot as plt
 
## for Deep-learing:
import tensorflow.keras

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
to_categorical([0, 1, 2, 3], num_classes=4)
from tensorflow.keras.optimizers import SGD 
from tensorflow.keras.callbacks import EarlyStopping
# from tensorflow.keras.utils import np_utils
import itertools
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import TimeDistributed
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Bidirectional
#import constraint
 
from sklearn.model_selection import train_test_split
from tensorflow.keras.regularizers import l2
 
 
###  Library for attention layers 
 
import pandas as pd
#import pyarrow.parquet as pq # Used to read the data
import os 
import numpy as np
from tensorflow.keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes
from tensorflow.keras.models import Model
#from tqdm import tqdm # Processing time measurement
from sklearn.model_selection import train_test_split 
from tensorflow.keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class
from tensorflow.keras import optimizers # Allow us to access the Adam class to modify some parameters
from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model
from tensorflow.keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting
 
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K
from tensorflow.keras import initializers
from tensorflow.keras import regularizers
import statistics
import gc


 
### Early stopping 
 
from tensorflow.keras.callbacks import EarlyStopping

 

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.compat.v1.Session(config=config)

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())


#sess = tf.compat.v1.Session(config=tf.ConfigProto(log_device_placement=True))
#from google.colab import drive
#drive.mount('/content/drive',force_remount=True)

### subject 6 ###

#55.34

w6=74.84*9.81
h6=1.8

IMU_6= loadtxt('subject_6_treamill_IMU.csv', delimiter=',')
IK_6= loadtxt('subject_6_treamill_IK.csv', delimiter=',')
ID_6= loadtxt('subject_6_treamill_ID.csv', delimiter=',')
GRF_6= loadtxt('subject_6_treamill_GRF.csv', delimiter=',')

subject_6_treadmill=np.concatenate((IMU_6,IK_6,ID_6/(w6*h6),GRF_6/w6),axis=1)

### subject 7 ###

w7=55.34*9.81
h7=1.65

IMU_7= loadtxt('subject_7_treamill_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_treamill_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_treamill_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_treamill_GRF.csv', delimiter=',')

subject_7_treadmill=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_levelground_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_levelground_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_levelground_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_levelground_GRF.csv', delimiter=',')

subject_7_levelground=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_ramp_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_ramp_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_ramp_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_ramp_GRF.csv', delimiter=',')

subject_7_ramp=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_stair_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_stair_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_stair_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_stair_GRF.csv', delimiter=',')

subject_7_stair=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

### subject 8 ###

w8=72.57*9.81
h8=1.74

IMU_8= loadtxt('subject_8_treamill_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_treamill_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_treamill_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_treamill_GRF.csv', delimiter=',')

subject_8_treadmill=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_levelground_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_levelground_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_levelground_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_levelground_GRF.csv', delimiter=',')

subject_8_levelground=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_ramp_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_ramp_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_ramp_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_ramp_GRF.csv', delimiter=',')

subject_8_ramp=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_stair_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_stair_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_stair_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_stair_GRF.csv', delimiter=',')

subject_8_stair=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)

### subject 9 ###

w9=63.5*9.81
h9=1.63

IMU_9= loadtxt('subject_9_treamill_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_treamill_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_treamill_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_treamill_GRF.csv', delimiter=',')

subject_9_treadmill=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_levelground_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_levelground_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_levelground_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_levelground_GRF.csv', delimiter=',')

subject_9_levelground=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_ramp_IMU_1.csv', delimiter=',')
IK_9= loadtxt('subject_9_ramp_IK_1.csv', delimiter=',')
ID_9= loadtxt('subject_9_ramp_ID_1.csv', delimiter=',')
GRF_9= loadtxt('subject_9_ramp_GRF_1.csv', delimiter=',')

subject_9_ramp_1=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_ramp_IMU_2.csv', delimiter=',')
IK_9= loadtxt('subject_9_ramp_IK_2.csv', delimiter=',')
ID_9= loadtxt('subject_9_ramp_ID_2.csv', delimiter=',')
GRF_9= loadtxt('subject_9_ramp_GRF_2.csv', delimiter=',')

subject_9_ramp_2=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


subject_9_ramp=np.concatenate((subject_9_ramp_1,subject_9_ramp_2),axis=0)



IMU_9= loadtxt('subject_9_stair_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_stair_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_stair_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_stair_GRF.csv', delimiter=',')

subject_9_stair=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)

### subject 10 ###

w10=83.91*9.81
h10=1.75

IMU_10= loadtxt('subject_10_treamill_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_treamill_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_treamill_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_treamill_GRF.csv', delimiter=',')

subject_10_treadmill=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)


IMU_10= loadtxt('subject_10_levelground_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_levelground_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_levelground_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_levelground_GRF.csv', delimiter=',')

subject_10_levelground=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)


IMU_10= loadtxt('subject_10_ramp_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_ramp_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_ramp_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_ramp_GRF.csv', delimiter=',')

subject_10_ramp=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)

IMU_10= loadtxt('subject_10_stair_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_stair_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_stair_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_stair_GRF.csv', delimiter=',')

subject_10_stair=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)

### subject 11 ###

#55.34
w11=77.11*9.81
h11=1.75

IMU_11= loadtxt('subject_11_treamill_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_treamill_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_treamill_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_treamill_GRF.csv', delimiter=',')

subject_11_treadmill=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_levelground_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_levelground_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_levelground_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_levelground_GRF.csv', delimiter=',')

subject_11_levelground=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_ramp_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_ramp_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_ramp_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_ramp_GRF.csv', delimiter=',')

subject_11_ramp=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_stair_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_stair_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_stair_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_stair_GRF.csv', delimiter=',')

subject_11_stair=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)

### subject 12 ###

#55.34
w12=86.18*9.81
h12=1.74

IMU_12= loadtxt('subject_12_treamill_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_treamill_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_treamill_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_treamill_GRF.csv', delimiter=',')

subject_12_treadmill=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_levelground_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_levelground_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_levelground_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_levelground_GRF.csv', delimiter=',')

subject_12_levelground=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_ramp_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_ramp_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_ramp_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_ramp_GRF.csv', delimiter=',')

subject_12_ramp=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_stair_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_stair_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_stair_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_stair_GRF.csv', delimiter=',')

subject_12_stair=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)

### subject 13 ###

#55.34
w13=58.97*9.81
h13=1.73

IMU_13= loadtxt('subject_13_treamill_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_treamill_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_treamill_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_treamill_GRF.csv', delimiter=',')

subject_13_treadmill=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)



IMU_13= loadtxt('subject_13_levelground_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_levelground_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_levelground_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_levelground_GRF.csv', delimiter=',')

subject_13_levelground=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)


IMU_13= loadtxt('subject_13_ramp_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_ramp_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_ramp_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_ramp_GRF.csv', delimiter=',')

subject_13_ramp=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)


IMU_13= loadtxt('subject_13_stair_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_stair_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_stair_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_stair_GRF.csv', delimiter=',')

subject_13_stair=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)

### subject 14 ###

#55.34
w14=58.41*9.81
h14=1.52

IMU_14= loadtxt('subject_14_treamill_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_treamill_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_treamill_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_treamill_GRF.csv', delimiter=',')

subject_14_treadmill=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)



IMU_14= loadtxt('subject_14_levelground_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_levelground_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_levelground_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_levelground_GRF.csv', delimiter=',')

subject_14_levelground=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)


IMU_14= loadtxt('subject_14_ramp_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_ramp_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_ramp_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_ramp_GRF.csv', delimiter=',')

subject_14_ramp=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)


IMU_14= loadtxt('subject_14_stair_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_stair_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_stair_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_stair_GRF.csv', delimiter=',')

subject_14_stair=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)

### subject 15 ###

#55.34
w15=96.16*9.81
h15=1.78

IMU_15= loadtxt('subject_15_treamill_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_treamill_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_treamill_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_treamill_GRF.csv', delimiter=',')

subject_15_treadmill=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_levelground_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_levelground_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_levelground_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_levelground_GRF.csv', delimiter=',')

subject_15_levelground=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_ramp_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_ramp_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_ramp_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_ramp_GRF.csv', delimiter=',')

subject_15_ramp=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_stair_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_stair_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_stair_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_stair_GRF.csv', delimiter=',')

subject_15_stair=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)

### subject 16 ###

#55.34
w16=55.79*9.81
h16=1.65

IMU_16= loadtxt('subject_16_treamill_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_treamill_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_treamill_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_treamill_GRF.csv', delimiter=',')

subject_16_treadmill=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)

IMU_16= loadtxt('subject_16_levelground_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_levelground_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_levelground_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_levelground_GRF.csv', delimiter=',')

subject_16_levelground=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)


IMU_16= loadtxt('subject_16_ramp_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_ramp_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_ramp_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_ramp_GRF.csv', delimiter=',')

subject_16_ramp=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)


IMU_16= loadtxt('subject_16_stair_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_stair_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_stair_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_stair_GRF.csv', delimiter=',')

subject_16_stair=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)

### subject 17 ###

#55.34
w17=61.23*9.81
h17=1.68

IMU_17= loadtxt('subject_17_treamill_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_treamill_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_treamill_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_treamill_GRF.csv', delimiter=',')

subject_17_treadmill=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_levelground_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_levelground_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_levelground_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_levelground_GRF.csv', delimiter=',')

subject_17_levelground=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_ramp_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_ramp_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_ramp_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_ramp_GRF.csv', delimiter=',')

subject_17_ramp=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_stair_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_stair_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_stair_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_stair_GRF.csv', delimiter=',')

subject_17_stair=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)

### subject 18 ###

#55.34
w18=60.13*9.81
h18=1.8

IMU_18= loadtxt('subject_18_treamill_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_treamill_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_treamill_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_treamill_GRF.csv', delimiter=',')

subject_18_treadmill=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_levelground_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_levelground_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_levelground_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_levelground_GRF.csv', delimiter=',')

subject_18_levelground=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_ramp_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_ramp_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_ramp_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_ramp_GRF.csv', delimiter=',')

subject_18_ramp=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_stair_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_stair_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_stair_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_stair_GRF.csv', delimiter=',')

subject_18_stair=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)

### subject 19 ###

#55.34
w19=68.04*9.81
h19=1.7

IMU_19= loadtxt('subject_19_treamill_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_treamill_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_treamill_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_treamill_GRF.csv', delimiter=',')

subject_19_treadmill=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_levelground_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_levelground_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_levelground_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_levelground_GRF.csv', delimiter=',')

subject_19_levelground=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_ramp_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_ramp_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_ramp_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_ramp_GRF.csv', delimiter=',')

subject_19_ramp=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_stair_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_stair_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_stair_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_stair_GRF.csv', delimiter=',')

subject_19_stair=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)

### subject 20 ###

#55.34
w20=68.04*9.81
h20=1.71

IMU_20= loadtxt('subject_20_treamill_IMU.csv', delimiter=',')
IK_20= loadtxt('subject_20_treamill_IK.csv', delimiter=',')
ID_20= loadtxt('subject_20_treamill_ID.csv', delimiter=',')
GRF_20= loadtxt('subject_20_treamill_GRF.csv', delimiter=',')

subject_20_treadmill=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_GRF.csv', delimiter=',')

# subject_20_levelground=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_GRF.csv', delimiter=',')

# subject_20_ramp=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_GRF.csv', delimiter=',')

# subject_20_stair=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)

### subject 21 ###

#55.34
w21=58.06*9.81
h21=1.57

IMU_21= loadtxt('subject_21_treamill_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_treamill_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_treamill_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_treamill_GRF.csv', delimiter=',')

subject_21_treadmill=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_levelground_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_levelground_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_levelground_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_levelground_GRF.csv', delimiter=',')

subject_21_levelground=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_ramp_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_ramp_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_ramp_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_ramp_GRF.csv', delimiter=',')

subject_21_ramp=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_stair_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_stair_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_stair_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_stair_GRF.csv', delimiter=',')

subject_21_stair=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)

### subject 23 ###

#55.34
w23=76.82*9.81
h23=1.8

IMU_23= loadtxt('subject_23_treamill_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_treamill_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_treamill_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_treamill_GRF.csv', delimiter=',')

subject_23_treadmill=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_levelground_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_levelground_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_levelground_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_levelground_GRF.csv', delimiter=',')

subject_23_levelground=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_ramp_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_ramp_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_ramp_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_ramp_GRF.csv', delimiter=',')

subject_23_ramp=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_stair_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_stair_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_stair_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_stair_GRF.csv', delimiter=',')

subject_23_stair=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)

### subject 24 ###

#55.34
w24=72.57*9.81
h24=1.73

IMU_24= loadtxt('subject_24_treamill_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_treamill_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_treamill_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_treamill_GRF.csv', delimiter=',')

subject_24_treadmill=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_levelground_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_levelground_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_levelground_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_levelground_GRF.csv', delimiter=',')

subject_24_levelground=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_ramp_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_ramp_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_ramp_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_ramp_GRF.csv', delimiter=',')

subject_24_ramp=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_stair_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_stair_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_stair_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_stair_GRF.csv', delimiter=',')

subject_24_stair=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)

### subject 25 ###

#55.34
w25=52.16*9.81
h25=1.63

IMU_25= loadtxt('subject_25_treamill_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_treamill_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_treamill_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_treamill_GRF.csv', delimiter=',')

subject_25_treadmill=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)

IMU_25= loadtxt('subject_25_levelground_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_levelground_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_levelground_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_levelground_GRF.csv', delimiter=',')

subject_25_levelground=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)


IMU_25= loadtxt('subject_25_ramp_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_ramp_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_ramp_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_ramp_GRF.csv', delimiter=',')

subject_25_ramp=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)


IMU_25= loadtxt('subject_25_stair_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_stair_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_stair_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_stair_GRF.csv', delimiter=',')

subject_25_stair=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)

### subject 27 ###

#55.34
w27=68.04*9.81
h27=1.7

IMU_27= loadtxt('subject_27_treamill_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_treamill_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_treamill_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_treamill_GRF.csv', delimiter=',')

subject_27_treadmill=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_levelground_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_levelground_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_levelground_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_levelground_GRF.csv', delimiter=',')

subject_27_levelground=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_ramp_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_ramp_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_ramp_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_ramp_GRF.csv', delimiter=',')

subject_27_ramp=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_stair_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_stair_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_stair_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_stair_GRF.csv', delimiter=',')

subject_27_stair=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)

### subject 28 ###

#55.34
w28=62.14*9.81
h28=1.69

IMU_28= loadtxt('subject_28_treamill_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_treamill_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_treamill_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_treamill_GRF.csv', delimiter=',')

subject_28_treadmill=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_levelground_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_levelground_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_levelground_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_levelground_GRF.csv', delimiter=',')

subject_28_levelground=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_ramp_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_ramp_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_ramp_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_ramp_GRF.csv', delimiter=',')

subject_28_ramp=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_stair_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_stair_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_stair_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_stair_GRF.csv', delimiter=',')

subject_28_stair=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)

### subject 30 ###

#55.34
w30=77.03*9.81
h30=1.77

IMU_30= loadtxt('subject_30_treamill_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_treamill_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_treamill_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_treamill_GRF.csv', delimiter=',')

subject_30_treadmill=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)



IMU_30= loadtxt('subject_30_levelground_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_levelground_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_levelground_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_levelground_GRF.csv', delimiter=',')

subject_30_levelground=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)


IMU_30= loadtxt('subject_30_ramp_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_ramp_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_ramp_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_ramp_GRF.csv', delimiter=',')

subject_30_ramp=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)


IMU_30= loadtxt('subject_30_stair_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_stair_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_stair_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_stair_GRF.csv', delimiter=',')

subject_30_stair=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)

gc.collect()



# Sensor 1- Sternum
# Sensor 2-Sacrum
# Sensor 3-R_thigh
# Sensor 4-L_thigh
# Sensor 5-R_shank
# Sensor 6-L_shank
# Sensor 7-R_dorsal
# Sensor 8-L_dorsal
 
 
 
def scene_results(train_dataset,test_dataset,test_dataset_normalization,path,scene):

  
  w=100
  
  # Sensor 1- Sternum
  # Sensor 2-Sacrum
  # Sensor 3-R_thigh
  # Sensor 4-L_thigh
  # Sensor 5-R_shank
  # Sensor 6-L_shank
  # Sensor 7-R_dorsal
  # Sensor 8-L_dorsal
   
   
  # Train features #
  train_1=train_dataset[:,1:7]
  train_2=train_dataset[:,7:13]
  train_3=train_dataset[:,13:19]
  train_4=train_dataset[:,19:25]
  
  
  from sklearn.preprocessing import StandardScaler
  
  
  x_train=train_1
  x_train=np.concatenate((train_3,train_2,train_1),axis=1)
  scale= StandardScaler()
   
  scaler = MinMaxScaler(feature_range=(0, 1))
  
  train_X_1_1=x_train
   
   
   
   
  # # Test features #
   
  test_1=test_dataset[:,1:7]
  test_2=test_dataset[:,7:13]
  test_3=test_dataset[:,13:19]
  test_4=test_dataset[:,19:25]
  
  
  
  #    ### Extra features  ###
    
   
  x_test=test_1
  
  x_test=np.concatenate((test_3,test_2,test_1),axis=1)
  
   
  test_X_1_1=x_test
  
   
  
  
    ### Label ###
      
  f=0
   
  
  
  y_1_1=train_dataset[:,(f+32):(f+33)]
  y_1_2=train_dataset[:,(f+35):(f+37)]
  y_1_3=train_dataset[:,(f+56):(f+58)]
  y_1_4=train_dataset[:,(f+65):(f+66)]
  y_1_5=train_dataset[:,(f+67):(f+68)]
  y_1_6=train_dataset[:,(f+74):(f+77)]
  y_1_7=train_dataset[:,(f+80):(f+83)]
  
  
  # train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5,y_1_6,y_1_7),axis=1)
  
  train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5,y_1_6),axis=1)
  
  # train_y_1_1=y_1_4
  
  
  y_2_1=test_dataset[:,(f+32):(f+33)]
  y_2_2=test_dataset[:,(f+35):(f+37)]
  y_2_3=test_dataset[:,(f+56):(f+58)]
  y_2_4=test_dataset[:,(f+65):(f+66)]
  y_2_5=test_dataset[:,(f+67):(f+68)]
  y_2_6=test_dataset[:,(f+74):(f+77)]
  y_2_7=test_dataset[:,(f+80):(f+83)]
  
  
  
   
  # test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5,y_2_6,y_2_7),axis=1)
  
  # test_y_1_1=y_2_4
  test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5,y_2_6),axis=1)
  
  train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)
  test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)
  
  train_dataset_1=pd.DataFrame(train_dataset_1)
  test_dataset_1=pd.DataFrame(test_dataset_1)
  
  train_dataset_1.dropna(axis=0,inplace=True)
  test_dataset_1.dropna(axis=0,inplace=True)
  
  train_dataset_1=np.array(train_dataset_1)
  test_dataset_1=np.array(test_dataset_1)
  
  train_dataset_sum = np. sum(train_dataset_1)
  array_has_nan = np. isnan(train_dataset_sum)
  
  print(array_has_nan)
  
  print(train_dataset_1.shape)
  
  
  
  train_X_1=train_dataset_1[:,0:18]
  test_X_1=test_dataset_1[:,0:18]
  
  train_y_1=train_dataset_1[:,18:25]
  test_y_1=test_dataset_1[:,18:25]
  
  
  
  L1=len(train_X_1)
  L2=len(test_X_1)
  # L3=len(validation_X_1)
  
  print(L1+L2)
   
  w=100
  
                     
  
   
   
  a1=L1//w
  b1=L1%w
   
  a2=L2//w
  b2=L2%w
  
  # a3=L3//w
  # b3=L3%w 
   
       #### Features ####
  train_X_2=train_X_1[L1-w+b1:L1,:]
  test_X_2=test_X_1[L2-w+b2:L2,:]
  # validation_X_2=validation_X_1[L3-w+b3:L3,:]
   
  
      #### Output ####
   
  train_y_2=train_y_1[L1-w+b1:L1,:]
  test_y_2=test_y_1[L2-w+b2:L2,:]
  # validation_y_2=validation_y_1[L3-w+b3:L3,:]
  
  
   
       #### Features ####
      
  train_X=np.concatenate((train_X_1,train_X_2),axis=0)
  test_X=np.concatenate((test_X_1,test_X_2),axis=0)
  # validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)
   
   
      #### Output ####
      
  train_y=np.concatenate((train_y_1,train_y_2),axis=0)
  test_y=np.concatenate((test_y_1,test_y_2),axis=0)
  # validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)
  
      
  print(train_y.shape) 
      #### Reshaping ####
  train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))
  test_X = test_X.reshape((a2+1,w,test_X.shape[1]))
  
  
  train_y_3_p= train_y.reshape((a1+1,w,7))
  test_y= test_y.reshape((a2+1,w,7))
  # Y_validation= validation_y.reshape((a3+1,w,6))
  
   
  
  # train_X_1D=train_X_3
  test_X_1D=test_X
  
  train_X_3=train_X_3_p
  train_y_3=train_y_3_p
  # print(train_X_4.shape,train_y_3.shape)
  
  train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)


  
  print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)
  
  features=6
  
  train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,3)
  test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,3)
  X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,3)
  #X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)
  
  
  s=test_X_1D.shape[0]*w
  
  print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)
  
  import tensorflow as tf
  # tensorflow import keras
  from tensorflow.keras import layers
  
  Bag_samples=train_X_2D.shape[0]
  print(Bag_samples)
  
  gc.collect()
  gc.collect()
  gc.collect()
  gc.collect()
  gc.collect()
  gc.collect()
  gc.collect()
  gc.collect()
  
  
  def normalization(test_dataset):  
    w=100
    
    # Sensor 1- Sternum
    # Sensor 2-Sacrum
    # Sensor 3-R_thigh
    # Sensor 4-L_thigh
    # Sensor 5-R_shank
    # Sensor 6-L_shank
    # Sensor 7-R_dorsal
    # Sensor 8-L_dorsal
     
     
    # Train features #
    train_1=train_dataset[:,1:7]
    train_2=train_dataset[:,7:13]
    train_3=train_dataset[:,13:19]
    train_4=train_dataset[:,19:25]
    
    
    from sklearn.preprocessing import StandardScaler
    
    
    x_train=train_1
    x_train=np.concatenate((train_3,train_2,train_1),axis=1)
    scale= StandardScaler()
     
    scaler = MinMaxScaler(feature_range=(0, 1))
    
    train_X_1_1=x_train
     
     
     
     
    # # Test features #
     
    test_1=test_dataset[:,1:7]
    test_2=test_dataset[:,7:13]
    test_3=test_dataset[:,13:19]
    test_4=test_dataset[:,19:25]
    
    
    
    #    ### Extra features  ###
      
     
    x_test=test_1
    
    x_test=np.concatenate((test_3,test_2,test_1),axis=1)
    
     
    test_X_1_1=x_test
    
     
    
    
      ### Label ###
        
    f=0
     
    
    
    y_1_1=train_dataset[:,(f+32):(f+33)]
    y_1_2=train_dataset[:,(f+35):(f+37)]
    y_1_3=train_dataset[:,(f+56):(f+58)]
    y_1_4=train_dataset[:,(f+65):(f+66)]
    y_1_5=train_dataset[:,(f+67):(f+68)]
    y_1_6=train_dataset[:,(f+74):(f+77)]
    y_1_7=train_dataset[:,(f+80):(f+83)]
    
    
    # train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5,y_1_6,y_1_7),axis=1)
    
    train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5,y_1_6),axis=1)
    
    # train_y_1_1=y_1_4
    
    
    y_2_1=test_dataset[:,(f+32):(f+33)]
    y_2_2=test_dataset[:,(f+35):(f+37)]
    y_2_3=test_dataset[:,(f+56):(f+58)]
    y_2_4=test_dataset[:,(f+65):(f+66)]
    y_2_5=test_dataset[:,(f+67):(f+68)]
    y_2_6=test_dataset[:,(f+74):(f+77)]
    y_2_7=test_dataset[:,(f+80):(f+83)]
    
    
    
     
    # test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5,y_2_6,y_2_7),axis=1)
    
    # test_y_1_1=y_2_4
    test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5,y_2_6),axis=1)
    
    train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)
    test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)
    
    train_dataset_1=pd.DataFrame(train_dataset_1)
    test_dataset_1=pd.DataFrame(test_dataset_1)
    
    train_dataset_1.dropna(axis=0,inplace=True)
    test_dataset_1.dropna(axis=0,inplace=True)
    
    train_dataset_1=np.array(train_dataset_1)
    test_dataset_1=np.array(test_dataset_1)
    
    train_dataset_sum = np. sum(train_dataset_1)
    array_has_nan = np. isnan(train_dataset_sum)
    
    print(array_has_nan)
    
    print(train_dataset_1.shape)
    
    
    
    train_X_1=train_dataset_1[:,0:18]
    test_X_1=test_dataset_1[:,0:18]
    
    train_y_1=train_dataset_1[:,18:25]
    test_y_1=test_dataset_1[:,18:25]
    
    
    
    L1=len(train_X_1)
    L2=len(test_X_1)
    # L3=len(validation_X_1)
    
    print(L1+L2)
     
    w=100
    
                       
    
     
     
    a1=L1//w
    b1=L1%w
     
    a2=L2//w
    b2=L2%w
    
    # a3=L3//w
    # b3=L3%w 
     
         #### Features ####
    train_X_2=train_X_1[L1-w+b1:L1,:]
    test_X_2=test_X_1[L2-w+b2:L2,:]
    # validation_X_2=validation_X_1[L3-w+b3:L3,:]
     
    
        #### Output ####
     
    train_y_2=train_y_1[L1-w+b1:L1,:]
    test_y_2=test_y_1[L2-w+b2:L2,:]
    # validation_y_2=validation_y_1[L3-w+b3:L3,:]
    
    
     
         #### Features ####
        
    train_X=np.concatenate((train_X_1,train_X_2),axis=0)
    test_X=np.concatenate((test_X_1,test_X_2),axis=0)
    # validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)
     
     
        #### Output ####
        
    train_y=np.concatenate((train_y_1,train_y_2),axis=0)
    test_y=np.concatenate((test_y_1,test_y_2),axis=0)
    # validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)
    
        
    print(train_y.shape) 
        #### Reshaping ####
    train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))
    test_X = test_X.reshape((a2+1,w,test_X.shape[1]))
    
    
    train_y_3_p= train_y.reshape((a1+1,w,7))
    test_y= test_y.reshape((a2+1,w,7))
    # Y_validation= validation_y.reshape((a3+1,w,6))
    
     
    
    # train_X_1D=train_X_3
    test_X_1D=test_X
    
    train_X_3=train_X_3_p
    train_y_3=train_y_3_p
    # print(train_X_4.shape,train_y_3.shape)
    
    train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)
  
  
    
    print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)
    
    features=6
    
    train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,3)
    test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,3)
    X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,3)
    #X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)
    
    
    s=test_X_1D.shape[0]*w
    
    print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)
    
    import tensorflow as tf
    # tensorflow import keras
    from tensorflow.keras import layers
    
    Bag_samples=train_X_2D.shape[0]
    print(Bag_samples)
    
    
    test_o=test_y.reshape((s,7))
     
    y_test_1=test_o[:,0]
    y_test_2=test_o[:,1]
    y_test_3=test_o[:,2]
    y_test_4=test_o[:,3]
    y_test_5=test_o[:,4]
    y_test_6=test_o[:,5]
    y_test_7=test_o[:,6]
    
    n1=max(y_test_1)-min(y_test_1)
    n2=max(y_test_2)-min(y_test_2)
    n3=max(y_test_3)-min(y_test_3)
    n4=max(y_test_4)-min(y_test_4)
    n5=max(y_test_5)-min(y_test_5)
    n6=max(y_test_6)-min(y_test_6)
    n7=max(y_test_7)-min(y_test_7)
    
    
    gc.collect()
    gc.collect()
    gc.collect()
    
    return n1,n2,n3,n4,n5,n6,n7

    
    
  
  
  
  """# Important Functions"""
  
  ##########################################################################################################################################################################################  
  ##########################################################################################################################################################################################    
  
    
    
    
  from sklearn.model_selection import KFold
  from sklearn.ensemble import GradientBoostingRegressor
  from sklearn.multioutput import MultiOutputRegressor
  import pickle
  from sklearn.linear_model import Ridge
  from sklearn.utils import resample
  
  """# Loss Function"""
  
  from keras import backend as K
  def correlation_coefficient_loss(y_true, y_pred):
      x = y_true
      y = y_pred
      mx = K.mean(x)
      my = K.mean(y)
      xm, ym = x-mx, y-my
      r_num = K.sum(tf.multiply(xm,ym))
      r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
      r = r_num / r_den
  
      #r = K.maximum(K.minimum(r, 1.0), -1.0)
  
      l1=K.sqrt(K.mean(K.square(y_pred - y_true)))
      #l2=1-K.square(r)
      l2=1-r
  
      l=l2
      return l
  
  from keras import backend as K
  def correlation_coefficient_loss_1(y_true, y_pred):
      x = y_true
      y = y_pred
      mx = K.mean(x)
      my = K.mean(y)
      xm, ym = x-mx, y-my
      r_num = K.sum(tf.multiply(xm,ym))
      r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
      r = r_num / r_den
  
      r = K.maximum(K.minimum(r, 1.0), -1.0)
  
      l1=K.sqrt(K.mean(K.square(y_pred - y_true)))
      l2=1-K.square(r)
  
      l=l1
      return l
  
  from keras import backend as K
  def correlation_coefficient_loss_joint(y_true, y_pred):
      x = y_true
      y = y_pred
      mx = K.mean(x)
      my = K.mean(y)
      xm, ym = x-mx, y-my
      r_num = K.sum(tf.multiply(xm,ym))
      r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
      r = r_num / r_den
  
      #r = K.maximum(K.minimum(r, 1.0), -1.0)
  
      l1=K.sqrt(K.mean(K.square(y_pred - y_true)))
      #l2=1-K.square(r)
      l2=1-r
  
      l=l1+l2
      return l
      
      
  custom_early_stopping=tf.keras.callbacks.EarlyStopping(
      monitor='val_loss',
      min_delta=0,
      patience=15,
      verbose=0,
      mode='auto',
      baseline=None,
      restore_best_weights=True
  ) 
  
  
  
  def RMSE_prediction(yhat_4,test_y,s):
   
    test_o=test_y.reshape((s,7))
    yhat=yhat_4.reshape((s,7))
    
    
    
    
    y_1_no=yhat[:,0]
    y_2_no=yhat[:,1]
    y_3_no=yhat[:,2]
    y_4_no=yhat[:,3]
    y_5_no=yhat[:,4]
    y_6_no=yhat[:,5]
    y_7_no=yhat[:,6]
    #y_8_no=yhat[:,7]
    #y_9_no=yhat[:,8]
    #y_10_no=yhat[:,9]
    
    
    
    
    y_test_1=test_o[:,0]
    y_test_2=test_o[:,1]
    y_test_3=test_o[:,2]
    y_test_4=test_o[:,3]
    y_test_5=test_o[:,4]
    y_test_6=test_o[:,5]
    y_test_7=test_o[:,6]
    #y_test_8=test_o[:,7]
    #y_test_9=test_o[:,8]
    #y_test_10=test_o[:,9]
    
    
    
    
    
    #print(y_1.shape,y_test_1.shape)
    
    
    
    cutoff=6
    fs=200
    order=4
    
    nyq = 0.5 * fs
    ## filtering data ##
    def butter_lowpass_filter(data, cutoff, fs, order):
        normal_cutoff = cutoff / nyq
        # Get the filter coefficients 
        b, a = butter(order, normal_cutoff, btype='low', analog=False)
        y = filtfilt(b, a, data)
        return y
    
    
    
    y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
    y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
    y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
    y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
    y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
    y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)
    y_7=butter_lowpass_filter(y_7_no, cutoff, fs, order)
    #y_8=butter_lowpass_filter(y_8_no, cutoff, fs, order)
    #y_9=butter_lowpass_filter(y_9_no, cutoff, fs, order)
    #y_10=butter_lowpass_filter(y_10_no, cutoff, fs, order)
    
    
    
    
    Z_1=y_1
    Z_2=y_2
    Z_3=y_3
    Z_4=y_4
    Z_5=y_5
    Z_6=y_6
    Z_7=y_7
    #Z_8=y_8
    #Z_9=y_9
    #Z_10=y_10
    
    
    
    ###calculate RMSE
    
    rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
    rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
    rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
    rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
    rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
    rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100
    rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100
    #rmse_8 =((np.sqrt(mean_squared_error(y_test_8,y_8)))/(max(y_test_8)-min(y_test_8)))*100
    #rmse_9 =((np.sqrt(mean_squared_error(y_test_9,y_9)))/(max(y_test_9)-min(y_test_9)))*100
    #rmse_10 =((np.sqrt(mean_squared_error(y_test_10,y_10)))/(max(y_test_10)-min(y_test_10)))*100
    
    
    print(rmse_1)
    print(rmse_2)
    print(rmse_3)
    print(rmse_4)
    print(rmse_5)
    print(rmse_6)
    print(rmse_7)
    #print(rmse_8)
    #print(rmse_9)
    #print(rmse_10)
    
    
    p_1=np.corrcoef(y_1, y_test_1)[0, 1]
    p_2=np.corrcoef(y_2, y_test_2)[0, 1]
    p_3=np.corrcoef(y_3, y_test_3)[0, 1]
    p_4=np.corrcoef(y_4, y_test_4)[0, 1]
    p_5=np.corrcoef(y_5, y_test_5)[0, 1]
    p_6=np.corrcoef(y_6, y_test_6)[0, 1]
    p_7=np.corrcoef(y_7, y_test_7)[0, 1]
    #p_8=np.corrcoef(y_8, y_test_8)[0, 1]
    #p_9=np.corrcoef(y_9, y_test_9)[0, 1]
    #p_10=np.corrcoef(y_10, y_test_10)[0, 1]
    
    
    print("\n") 
    print(p_1)
    print(p_2)
    print(p_3)
    print(p_4)
    print(p_5)
    print(p_6)
    print(p_7)
    #print(p_8)
    #print(p_9)
    #print(p_10)
    
    
                ### Correlation ###
    p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7])
    
    
    
    
        #### Mean and standard deviation ####
    
    rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7])
    
        #### Mean and standard deviation ####
    m=statistics.mean(rmse)
    SD=statistics.stdev(rmse)
    print('Mean: %.3f' % m,'+/- %.3f' %SD)
     
    m_c=statistics.mean(p)
    SD_c=statistics.stdev(p)
    print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
  
    return rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7
  
  
  ############################################################################################################################################################################################################################################################################################################################################################################################################################################################################
  
  
  def PCC_prediction(yhat_4,test_y,s):
   
    test_o=test_y.reshape((s,7))
    yhat=yhat_4.reshape((s,7))
    
    
    
    y_1_no=yhat[:,0]
    y_2_no=yhat[:,1]
    y_3_no=yhat[:,2]
    y_4_no=yhat[:,3]
    y_5_no=yhat[:,4]
    y_6_no=yhat[:,5]
    y_7_no=yhat[:,6]
    
    
    y_test_1=test_o[:,0]
    y_test_2=test_o[:,1]
    y_test_3=test_o[:,2]
    y_test_4=test_o[:,3]
    y_test_5=test_o[:,4]
    y_test_6=test_o[:,5]
    y_test_7=test_o[:,6]
    
    
    
    cutoff=6
    fs=200
    order=4
    
    nyq = 0.5 * fs
    ## filtering data ##
    def butter_lowpass_filter(data, cutoff, fs, order):
        normal_cutoff = cutoff / nyq
        # Get the filter coefficients 
        b, a = butter(order, normal_cutoff, btype='low', analog=False)
        y = filtfilt(b, a, data)
        return y
    
    
    
    y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
    y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
    y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
    y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
    y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
    y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)
    y_7=butter_lowpass_filter(y_7_no, cutoff, fs, order)
    
    
    Y_1=y_1
    Y_2=y_2
    Y_3=y_3
    Y_4=y_4
    Y_5=y_5
    Y_6=y_6
    Y_7=y_7
    
    
    ###calculate RMSE
    
    rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
    rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
    rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
    rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
    rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
    rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100
    rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100
    
    
    print(rmse_1)
    print(rmse_2)
    print(rmse_3)
    print(rmse_4)
    print(rmse_5)
    print(rmse_6)
    print(rmse_7)
    
    
    p_1=np.corrcoef(y_1, y_test_1)[0, 1]
    p_2=np.corrcoef(y_2, y_test_2)[0, 1]
    p_3=np.corrcoef(y_3, y_test_3)[0, 1]
    p_4=np.corrcoef(y_4, y_test_4)[0, 1]
    p_5=np.corrcoef(y_5, y_test_5)[0, 1]
    p_6=np.corrcoef(y_6, y_test_6)[0, 1]
    p_7=np.corrcoef(y_7, y_test_7)[0, 1]
    
    
    print("\n") 
    print(p_1)
    print(p_2)
    print(p_3)
    print(p_4)
    print(p_5)
    print(p_6)
    print(p_7)
    
    
    
                ### Correlation ###
    p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7])
    
    
    
    
        #### Mean and standard deviation ####
    
    rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7])
    
        #### Mean and standard deviation ####
    m=statistics.mean(rmse)
    SD=statistics.stdev(rmse)
    print('Mean: %.3f' % m,'+/- %.3f' %SD)
     
    m_c=statistics.mean(p)
    SD_c=statistics.stdev(p)
    print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
    
    
    return rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5,Y_6,Y_7
    
  
  ############################################################################################################################################################################################################################################################################################################################################################################################################################################################################
  
  
  def estimate_coef(x, y):
      # number of observations/points
      n = np.size(x)
    
      # mean of x and y vector
      m_x = np.mean(x)
      m_y = np.mean(y)
    
      # calculating cross-deviation and deviation about x
      SS_xy = np.sum(y*x) - n*m_y*m_x
      SS_xx = np.sum(x*x) - n*m_x*m_x
    
      # calculating regression coefficients
      b_1 = SS_xy / SS_xx
      b_0 = m_y - b_1*m_x
    
      return (b_0, b_1)  
  
  
  ############################################################################################################################################################################################################################################################################################################################################################################################################################################################################
  
  
  
  def DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Y_6,Y_7,Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7):
  
    a_1,b_1=estimate_coef(Y_1,Z_1)
    a_2,b_2=estimate_coef(Y_2,Z_2)
    a_3,b_3=estimate_coef(Y_3,Z_3)
    a_4,b_4=estimate_coef(Y_4,Z_4)
    a_5,b_5=estimate_coef(Y_5,Z_5)
    a_6,b_6=estimate_coef(Y_6,Z_6)
    a_7,b_7=estimate_coef(Y_7,Z_7)
    
    #### All 16 angles prediction  ####
    
     
    test_o=test_y.reshape((s,7))
    yhat=yhat_4.reshape((s,7))
    
    
    y_1_no=yhat[:,0]
    y_2_no=yhat[:,1]
    y_3_no=yhat[:,2]
    y_4_no=yhat[:,3]
    y_5_no=yhat[:,4]
    y_6_no=yhat[:,5]
    y_7_no=yhat[:,6]
    
    
    
    y_test_1=test_o[:,0]
    y_test_2=test_o[:,1]
    y_test_3=test_o[:,2]
    y_test_4=test_o[:,3]
    y_test_5=test_o[:,4]
    y_test_6=test_o[:,5]
    y_test_7=test_o[:,6]
    
    
    cutoff=6
    fs=200
    order=4
    
    nyq = 0.5 * fs
    ## filtering data ##
    def butter_lowpass_filter(data, cutoff, fs, order):
        normal_cutoff = cutoff / nyq
        # Get the filter coefficients 
        b, a = butter(order, normal_cutoff, btype='low', analog=False)
        y = filtfilt(b, a, data)
        return y
    
    
    
    y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
    y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
    y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
    y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
    y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
    y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)
    y_7=butter_lowpass_filter(y_7_no, cutoff, fs, order)
    
    
    
    y_1=y_1*b_1+a_1
    y_2=y_2*b_2+a_2
    y_3=y_3*b_3+a_3
    y_4=y_4*b_4+a_4
    y_5=y_5*b_5+a_5
    y_6=y_6*b_6+a_6
    y_7=y_7*b_7+a_7
    
    

    
    
    ###calculate RMSE
    
    rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
    rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
    rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
    rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
    rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
    rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100
    rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100
    
    
    
    
    
    print(rmse_1)
    print(rmse_2)
    print(rmse_3)
    print(rmse_4)
    print(rmse_5)
    print(rmse_6)
    print(rmse_7)
    
    
    
    p_1=np.corrcoef(y_1, y_test_1)[0, 1]
    p_2=np.corrcoef(y_2, y_test_2)[0, 1]
    p_3=np.corrcoef(y_3, y_test_3)[0, 1]
    p_4=np.corrcoef(y_4, y_test_4)[0, 1]
    p_5=np.corrcoef(y_5, y_test_5)[0, 1]
    p_6=np.corrcoef(y_6, y_test_6)[0, 1]
    p_7=np.corrcoef(y_7, y_test_7)[0, 1]
    
    
    print("\n") 
    print(p_1)
    print(p_2)
    print(p_3)
    print(p_4)
    print(p_5)
    print(p_6)
    print(p_7)
    
    
                ### Correlation ###
    p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7])
    
    
    
        #### Mean and standard deviation ####
    
    rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7])
    
        #### Mean and standard deviation ####
    m=statistics.mean(rmse)
    SD=statistics.stdev(rmse)
    print('Mean: %.3f' % m,'+/- %.3f' %SD)
     
    m_c=statistics.mean(p)
    SD_c=statistics.stdev(p)
    print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
    
    y_bag=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6,y_7))
    y_bag=np.transpose(y_bag)
    
    return y_bag
  
  
  
  
  
  ############################################################################################################################################################################################################################################################################################################################################################################################################################################################################
  
  
  
  i=0
  ix = [i for i in range(len(train_X_2D))]
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  # test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_1=train_X_1D[train_ix]
  train_X_2D_1=train_X_2D[train_ix]
  train_y_5_1=train_y_5[train_ix]
  
    
    
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  # test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_2=train_X_1D[train_ix]
  train_X_2D_2=train_X_2D[train_ix]
  train_y_5_2=train_y_5[train_ix]
  
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  # test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_3=train_X_1D[train_ix]
  train_X_2D_3=train_X_2D[train_ix]
  train_y_5_3=train_y_5[train_ix]
  
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  # test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_4=train_X_1D[train_ix]
  train_X_2D_4=train_X_2D[train_ix]
  train_y_5_4=train_y_5[train_ix]
  
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  # test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_5=train_X_1D[train_ix]
  train_X_2D_5=train_X_2D[train_ix]
  train_y_5_5=train_y_5[train_ix]
  
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_6=train_X_1D[train_ix]
  train_X_2D_6=train_X_2D[train_ix]
  train_y_5_6=train_y_5[train_ix]
  
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_7=train_X_1D[train_ix]
  train_X_2D_7=train_X_2D[train_ix]
  train_y_5_7=train_y_5[train_ix]
  
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_8=train_X_1D[train_ix]
  train_X_2D_8=train_X_2D[train_ix]
  train_y_5_8=train_y_5[train_ix]
  
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_9=train_X_1D[train_ix]
  train_X_2D_9=train_X_2D[train_ix]
  train_y_5_9=train_y_5[train_ix]
  
  train_ix = resample(ix, replace=True, n_samples=Bag_samples,random_state=None)
  test_ix = [x for x in ix if x not in train_ix]
  train_X_1D_10=train_X_1D[train_ix]
  train_X_2D_10=train_X_2D[train_ix]
  train_y_5_10=train_y_5[train_ix]
  
  
  def Bagging_prediction(yhat_4,test_y,s):
   
    test_o=test_y.reshape((s,7))
    yhat=yhat_4.reshape((s,7))
    
        
    y_1=yhat[:,0]
    y_2=yhat[:,1]
    y_3=yhat[:,2]
    y_4=yhat[:,3]
    y_5=yhat[:,4]
    y_6=yhat[:,5]
    y_7=yhat[:,6]
    #y_8_no=yhat[:,7]
    #y_9_no=yhat[:,8]
    #y_10_no=yhat[:,9]
    
    
    
    
    y_test_1=test_o[:,0]
    y_test_2=test_o[:,1]
    y_test_3=test_o[:,2]
    y_test_4=test_o[:,3]
    y_test_5=test_o[:,4]
    y_test_6=test_o[:,5]
    y_test_7=test_o[:,6]
    #y_test_8=test_o[:,7]
    #y_test_9=test_o[:,8]
    #y_test_10=test_o[:,9]
    
    
    
    
    
    #print(y_1.shape,y_test_1.shape)
    
    
    
    cutoff=6
    fs=200
    order=4
    
    nyq = 0.5 * fs
    ## filtering data ##
    def butter_lowpass_filter(data, cutoff, fs, order):
        normal_cutoff = cutoff / nyq
        # Get the filter coefficients 
        b, a = butter(order, normal_cutoff, btype='low', analog=False)
        y = filtfilt(b, a, data)
        return y
    
    
   
    
    
    
    
    Z_1=y_1
    Z_2=y_2
    Z_3=y_3
    Z_4=y_4
    Z_5=y_5
    Z_6=y_6
    Z_7=y_7
    #Z_8=y_8
    #Z_9=y_9
    #Z_10=y_10
    
    
    
    y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6,y_7))
    y_bag_1=np.transpose(y_bag_1)
    

    
    n1,n2,n3,n4,n5,n6,n7=normalization(test_dataset_normalization)
    
    ###calculate RMSE
    
    rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(n1))*100
    rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(n2))*100
    rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(n3))*100
    rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(n4))*100
    rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(n5))*100
    rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(n6))*100
    rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(n7))*100
    #rmse_8 =((np.sqrt(mean_squared_error(y_test_8,y_8)))/(max(y_test_8)-min(y_test_8)))*100
    #rmse_9 =((np.sqrt(mean_squared_error(y_test_9,y_9)))/(max(y_test_9)-min(y_test_9)))*100
    #rmse_10 =((np.sqrt(mean_squared_error(y_test_10,y_10)))/(max(y_test_10)-min(y_test_10)))*100
    
    
    print(rmse_1)
    print(rmse_2)
    print(rmse_3)
    print(rmse_4)
    print(rmse_5)
    print(rmse_6)
    print(rmse_7)
    #print(rmse_8)
    #print(rmse_9)
    #print(rmse_10)
    
    
    p_1=np.corrcoef(y_1, y_test_1)[0, 1]
    p_2=np.corrcoef(y_2, y_test_2)[0, 1]
    p_3=np.corrcoef(y_3, y_test_3)[0, 1]
    p_4=np.corrcoef(y_4, y_test_4)[0, 1]
    p_5=np.corrcoef(y_5, y_test_5)[0, 1]
    p_6=np.corrcoef(y_6, y_test_6)[0, 1]
    p_7=np.corrcoef(y_7, y_test_7)[0, 1]
    #p_8=np.corrcoef(y_8, y_test_8)[0, 1]
    #p_9=np.corrcoef(y_9, y_test_9)[0, 1]
    #p_10=np.corrcoef(y_10, y_test_10)[0, 1]
    
    
    print("\n") 
    print(p_1)
    print(p_2)
    print(p_3)
    print(p_4)
    print(p_5)
    print(p_6)
    print(p_7)
    #print(p_8)
    #print(p_9)
    #print(p_10)
    
    
                ### Correlation ###
    p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7])
    
    
    
    
        #### Mean and standard deviation ####
    
    rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7])
    
        #### Mean and standard deviation ####
    m=statistics.mean(rmse)
    SD=statistics.stdev(rmse)
    print('Mean: %.3f' % m,'+/- %.3f' %SD)
     
    m_c=statistics.mean(p)
    SD_c=statistics.stdev(p)
    print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
  
    return rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7
  
  
  
  ############################################################################################################################################################################################################################################################################################################################################################################################################################################################################
  

  
  """## Kinetics-FM-Net"""
  
  
  def Results_output(bag_number,Features,sensor,train_X_1D,train_X_2D,train_y_5):
  
  

    filename=path+'model_Kinetics_FM_'+bag_number+'.h5'
    model_1=tf.keras.models.load_model(filename, custom_objects={'correlation_coefficient_loss_1': correlation_coefficient_loss_1})
  
  
    gc.collect()
  
  
    [yhat_1,yhat_2,yhat_3,yhat_4] = model_1.predict([test_X_1D,test_X_2D])
  
  
    rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=RMSE_prediction(yhat_4,test_y,s)
  
  
    
    filename=path+'model_Kinetics_FM_PCC_'+bag_number+'.h5'
    model_2=tf.keras.models.load_model(filename, custom_objects={'correlation_coefficient_loss': correlation_coefficient_loss})
  
  
    gc.collect()
  
    [yhat_1,yhat_2,yhat_3,yhat_4] = model_2.predict([test_X_1D,test_X_2D])
  
    rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5,Y_6,Y_7=PCC_prediction(yhat_4,test_y,s)
    y_bag_1=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Y_6,Y_7,Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7)
  
  
    gc.collect()
    gc.collect()
    gc.collect()
    gc.collect()
    
    return y_bag_1
    
    
  
  
  y_bag_1=Results_output('1',18,3,train_X_1D_1,train_X_2D_1,train_y_5_1)
  y_bag_2=Results_output('2',18,3,train_X_1D_2,train_X_2D_2,train_y_5_2)
  y_bag_3=Results_output('3',18,3,train_X_1D_3,train_X_2D_3,train_y_5_3)
  y_bag_4=Results_output('4',18,3,train_X_1D_4,train_X_2D_4,train_y_5_4)
  y_bag_5=Results_output('5',18,3,train_X_1D_5,train_X_2D_5,train_y_5_5)
  y_bag_6=Results_output('6',18,3,train_X_1D_6,train_X_2D_6,train_y_5_6)
  y_bag_7=Results_output('7',18,3,train_X_1D_7,train_X_2D_7,train_y_5_7)
  y_bag_8=Results_output('8',18,3,train_X_1D_8,train_X_2D_8,train_y_5_8)
  y_bag_9=Results_output('9',18,3,train_X_1D_9,train_X_2D_9,train_y_5_9)
  y_bag_10=Results_output('10',18,3,train_X_1D_10,train_X_2D_10,train_y_5_10)
                                                 
  
    
  #######################################################################################################################################################################################################################################
  #######################################################################################################################################################################################################################################

  yhat_10=np.mean((y_bag_1,y_bag_2,y_bag_3,y_bag_4,y_bag_5,y_bag_6,y_bag_7,y_bag_8,y_bag_9,y_bag_10),axis=0)
  

  rmse_bag_10,p_bag_10,Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7=Bagging_prediction(yhat_10,test_y,s)
  

  ablation_10=np.hstack([rmse_bag_10,p_bag_10])
    
  

  
  
  return ablation_10
  
  
######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################


train_dataset_treadmill=np.concatenate((subject_7_treadmill,subject_8_treadmill,subject_9_treadmill,
                              subject_10_treadmill,subject_11_treadmill,subject_12_treadmill,subject_13_treadmill,\
                              subject_14_treadmill,subject_15_treadmill,subject_16_treadmill,subject_17_treadmill,subject_18_treadmill,\
                              subject_19_treadmill,subject_21_treadmill,subject_23_treadmill,subject_24_treadmill,subject_25_treadmill,subject_27_treadmill,subject_28_treadmill),axis=0)


train_dataset_levelground=np.concatenate((subject_7_levelground,subject_8_levelground,subject_9_levelground,
                              subject_10_levelground,subject_11_levelground,subject_12_levelground,subject_13_levelground,\
                              subject_14_levelground,subject_15_levelground,subject_16_levelground,subject_17_levelground,subject_18_levelground,\
                              subject_19_levelground,subject_21_levelground,subject_23_levelground,subject_24_levelground,subject_25_levelground,subject_27_levelground,subject_28_levelground),axis=0)


train_dataset_ramp=np.concatenate((subject_7_ramp,subject_8_ramp,subject_9_ramp,
                              subject_10_ramp,subject_11_ramp,subject_12_ramp,subject_13_ramp,\
                              subject_14_ramp,subject_15_ramp,subject_16_ramp,subject_17_ramp,subject_18_ramp,\
                              subject_19_ramp,subject_21_ramp,subject_23_ramp,subject_24_ramp,subject_25_ramp,subject_27_ramp,subject_28_ramp),axis=0)


train_dataset_stair=np.concatenate((subject_7_stair,subject_8_stair,subject_9_stair,
                              subject_10_stair,subject_11_stair,subject_12_stair,subject_13_stair,\
                              subject_14_stair,subject_15_stair,subject_16_stair,subject_17_stair,subject_18_stair,\
                              subject_19_stair,subject_21_stair,subject_23_stair,subject_24_stair,subject_25_stair,subject_27_stair,subject_28_stair),axis=0)


train_dataset=np.concatenate((train_dataset_treadmill,train_dataset_levelground,train_dataset_ramp,train_dataset_stair),axis=0)

print(train_dataset_levelground.shape)
print(train_dataset_treadmill.shape)



import os 
 



######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################

from numpy import savetxt


path='/home/sanzidpr/JBHI_revision/Bagging/Subject09/'
  
test_dataset_normalization=np.concatenate((subject_9_treadmill,subject_9_levelground,subject_9_ramp,subject_9_stair),axis=0)

treadmill=subject_9_treadmill
levelground=subject_9_levelground
ramp=subject_9_ramp
stair=subject_9_stair
  

result_treadmill=scene_results(train_dataset,treadmill,test_dataset_normalization,path,'Treadmill')

#savetxt(path+'treadmill_plot.csv', result_treadmill, delimiter=',')

result_levelground=scene_results(train_dataset,levelground,test_dataset_normalization,path,'Levelground')

#savetxt(path+'levelground_plot.csv', result_levelground, delimiter=',')

result_ramp=scene_results(train_dataset,ramp,test_dataset_normalization,path,'Ramp')

#savetxt(path+'ramp_plot.csv', result_ramp, delimiter=',')

result_stair=scene_results(train_dataset,stair,test_dataset_normalization,path,'Stair')

#savetxt(path+'stair_plot.csv', result_stair, delimiter=',')



scene_results=np.vstack([result_treadmill,result_levelground,result_ramp,result_stair])

savetxt(path+'Subject_09_scene_results_v2.csv', scene_results, delimiter=',')

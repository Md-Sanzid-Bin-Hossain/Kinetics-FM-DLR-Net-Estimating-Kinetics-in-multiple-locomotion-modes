# -*- coding: utf-8 -*-
"""Future_Gait_DLR_Net_public_data_Stair_slope_Kinematics_kinetics_Public_dataset_kinematics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/116DLyfghavEPPeefrKabrJvohcuH2_xs
"""

# Let's import all packages that we may need:
# Let's import all packages that we may need:
import numpy
import tensorflow as tf
import statistics 
from numpy import loadtxt
import matplotlib.pyplot as plt
import pandas
import math
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import GRU,LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statistics import stdev 
import math
# import tsfel

 
import numpy as np

from scipy.signal import butter,filtfilt
 
import sys 
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
import pandas
import matplotlib.pyplot as plt
 
## for Deep-learing:
import tensorflow.keras
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
to_categorical([0, 1, 2, 3], num_classes=4)
from tensorflow.keras.optimizers import SGD 
from tensorflow.keras.callbacks import EarlyStopping
# from tensorflow.keras.utils import np_utils
import itertools
from tensorflow.keras.layers import LSTM
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Dropout
from keras.layers import TimeDistributed
from keras.layers import Flatten
from keras.layers import Bidirectional
#import constraint
 
from sklearn.model_selection import train_test_split
from keras.regularizers import l2
 
 
###  Library for attention layers 
 
import pandas as pd
#import pyarrow.parquet as pq # Used to read the data
import os 
import numpy as np
from tensorflow.keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes
from tensorflow.keras.models import Model
from tqdm import tqdm # Processing time measurement
from sklearn.model_selection import train_test_split 
from tensorflow.keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class
from tensorflow.keras import optimizers # Allow us to access the Adam class to modify some parameters
from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model
from tensorflow.keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting
 
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K
from tensorflow.keras import initializers
from tensorflow.keras import regularizers
import statistics
import gc

import wget
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense
from tqdm.notebook import tqdm

from tcn import TCN
 
### Early stopping 
 
from tensorflow.keras.callbacks import EarlyStopping

 

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.compat.v1.Session(config=config)

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())


#sess = tf.compat.v1.Session(config=tf.ConfigProto(log_device_placement=True))
#from google.colab import drive
#drive.mount('/content/drive',force_remount=True)

### subject 6 ###

#55.34

w6=74.84*9.81
h6=1.8

IMU_6= loadtxt('subject_6_treamill_IMU.csv', delimiter=',')
IK_6= loadtxt('subject_6_treamill_IK.csv', delimiter=',')
ID_6= loadtxt('subject_6_treamill_ID.csv', delimiter=',')
GRF_6= loadtxt('subject_6_treamill_GRF.csv', delimiter=',')

subject_6_treadmill=np.concatenate((IMU_6,IK_6,ID_6/(w6*h6),GRF_6/w6),axis=1)

### subject 7 ###

w7=55.34*9.81
h7=1.65

IMU_7= loadtxt('subject_7_treamill_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_treamill_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_treamill_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_treamill_GRF.csv', delimiter=',')

subject_7_treadmill=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_levelground_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_levelground_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_levelground_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_levelground_GRF.csv', delimiter=',')

subject_7_levelground=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_ramp_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_ramp_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_ramp_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_ramp_GRF.csv', delimiter=',')

subject_7_ramp=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_stair_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_stair_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_stair_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_stair_GRF.csv', delimiter=',')

subject_7_stair=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

### subject 8 ###

w8=72.57*9.81
h8=1.74

IMU_8= loadtxt('subject_8_treamill_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_treamill_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_treamill_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_treamill_GRF.csv', delimiter=',')

subject_8_treadmill=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_levelground_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_levelground_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_levelground_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_levelground_GRF.csv', delimiter=',')

subject_8_levelground=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_ramp_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_ramp_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_ramp_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_ramp_GRF.csv', delimiter=',')

subject_8_ramp=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_stair_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_stair_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_stair_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_stair_GRF.csv', delimiter=',')

subject_8_stair=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)

### subject 9 ###

w9=63.5*9.81
h9=1.63

IMU_9= loadtxt('subject_9_treamill_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_treamill_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_treamill_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_treamill_GRF.csv', delimiter=',')

subject_9_treadmill=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_levelground_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_levelground_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_levelground_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_levelground_GRF.csv', delimiter=',')

subject_9_levelground=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_ramp_IMU_1.csv', delimiter=',')
IK_9= loadtxt('subject_9_ramp_IK_1.csv', delimiter=',')
ID_9= loadtxt('subject_9_ramp_ID_1.csv', delimiter=',')
GRF_9= loadtxt('subject_9_ramp_GRF_1.csv', delimiter=',')

subject_9_ramp_1=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_ramp_IMU_2.csv', delimiter=',')
IK_9= loadtxt('subject_9_ramp_IK_2.csv', delimiter=',')
ID_9= loadtxt('subject_9_ramp_ID_2.csv', delimiter=',')
GRF_9= loadtxt('subject_9_ramp_GRF_2.csv', delimiter=',')

subject_9_ramp_2=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


subject_9_ramp=np.concatenate((subject_9_ramp_1,subject_9_ramp_2),axis=0)



IMU_9= loadtxt('subject_9_stair_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_stair_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_stair_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_stair_GRF.csv', delimiter=',')

subject_9_stair=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)

### subject 10 ###

w10=83.91*9.81
h10=1.75

IMU_10= loadtxt('subject_10_treamill_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_treamill_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_treamill_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_treamill_GRF.csv', delimiter=',')

subject_10_treadmill=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)


IMU_10= loadtxt('subject_10_levelground_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_levelground_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_levelground_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_levelground_GRF.csv', delimiter=',')

subject_10_levelground=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)


IMU_10= loadtxt('subject_10_ramp_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_ramp_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_ramp_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_ramp_GRF.csv', delimiter=',')

subject_10_ramp=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)

IMU_10= loadtxt('subject_10_stair_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_stair_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_stair_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_stair_GRF.csv', delimiter=',')

subject_10_stair=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)

### subject 11 ###

#55.34
w11=77.11*9.81
h11=1.75

IMU_11= loadtxt('subject_11_treamill_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_treamill_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_treamill_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_treamill_GRF.csv', delimiter=',')

subject_11_treadmill=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_levelground_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_levelground_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_levelground_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_levelground_GRF.csv', delimiter=',')

subject_11_levelground=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_ramp_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_ramp_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_ramp_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_ramp_GRF.csv', delimiter=',')

subject_11_ramp=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_stair_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_stair_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_stair_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_stair_GRF.csv', delimiter=',')

subject_11_stair=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)

### subject 12 ###

#55.34
w12=86.18*9.81
h12=1.74

IMU_12= loadtxt('subject_12_treamill_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_treamill_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_treamill_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_treamill_GRF.csv', delimiter=',')

subject_12_treadmill=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_levelground_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_levelground_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_levelground_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_levelground_GRF.csv', delimiter=',')

subject_12_levelground=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_ramp_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_ramp_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_ramp_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_ramp_GRF.csv', delimiter=',')

subject_12_ramp=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_stair_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_stair_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_stair_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_stair_GRF.csv', delimiter=',')

subject_12_stair=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)

### subject 13 ###

#55.34
w13=58.97*9.81
h13=1.73

IMU_13= loadtxt('subject_13_treamill_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_treamill_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_treamill_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_treamill_GRF.csv', delimiter=',')

subject_13_treadmill=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)



IMU_13= loadtxt('subject_13_levelground_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_levelground_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_levelground_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_levelground_GRF.csv', delimiter=',')

subject_13_levelground=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)


IMU_13= loadtxt('subject_13_ramp_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_ramp_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_ramp_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_ramp_GRF.csv', delimiter=',')

subject_13_ramp=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)


IMU_13= loadtxt('subject_13_stair_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_stair_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_stair_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_stair_GRF.csv', delimiter=',')

subject_13_stair=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)

### subject 14 ###

#55.34
w14=58.41*9.81
h14=1.52

IMU_14= loadtxt('subject_14_treamill_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_treamill_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_treamill_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_treamill_GRF.csv', delimiter=',')

subject_14_treadmill=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)



IMU_14= loadtxt('subject_14_levelground_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_levelground_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_levelground_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_levelground_GRF.csv', delimiter=',')

subject_14_levelground=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)


IMU_14= loadtxt('subject_14_ramp_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_ramp_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_ramp_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_ramp_GRF.csv', delimiter=',')

subject_14_ramp=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)


IMU_14= loadtxt('subject_14_stair_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_stair_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_stair_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_stair_GRF.csv', delimiter=',')

subject_14_stair=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)

### subject 15 ###

#55.34
w15=96.16*9.81
h15=1.78

IMU_15= loadtxt('subject_15_treamill_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_treamill_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_treamill_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_treamill_GRF.csv', delimiter=',')

subject_15_treadmill=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_levelground_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_levelground_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_levelground_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_levelground_GRF.csv', delimiter=',')

subject_15_levelground=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_ramp_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_ramp_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_ramp_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_ramp_GRF.csv', delimiter=',')

subject_15_ramp=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_stair_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_stair_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_stair_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_stair_GRF.csv', delimiter=',')

subject_15_stair=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)

### subject 16 ###

#55.34
w16=55.79*9.81
h16=1.65

IMU_16= loadtxt('subject_16_treamill_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_treamill_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_treamill_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_treamill_GRF.csv', delimiter=',')

subject_16_treadmill=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)

IMU_16= loadtxt('subject_16_levelground_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_levelground_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_levelground_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_levelground_GRF.csv', delimiter=',')

subject_16_levelground=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)


IMU_16= loadtxt('subject_16_ramp_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_ramp_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_ramp_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_ramp_GRF.csv', delimiter=',')

subject_16_ramp=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)


IMU_16= loadtxt('subject_16_stair_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_stair_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_stair_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_stair_GRF.csv', delimiter=',')

subject_16_stair=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)

### subject 17 ###

#55.34
w17=61.23*9.81
h17=1.68

IMU_17= loadtxt('subject_17_treamill_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_treamill_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_treamill_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_treamill_GRF.csv', delimiter=',')

subject_17_treadmill=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_levelground_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_levelground_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_levelground_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_levelground_GRF.csv', delimiter=',')

subject_17_levelground=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_ramp_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_ramp_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_ramp_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_ramp_GRF.csv', delimiter=',')

subject_17_ramp=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_stair_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_stair_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_stair_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_stair_GRF.csv', delimiter=',')

subject_17_stair=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)

### subject 18 ###

#55.34
w18=60.13*9.81
h18=1.8

IMU_18= loadtxt('subject_18_treamill_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_treamill_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_treamill_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_treamill_GRF.csv', delimiter=',')

subject_18_treadmill=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_levelground_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_levelground_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_levelground_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_levelground_GRF.csv', delimiter=',')

subject_18_levelground=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_ramp_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_ramp_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_ramp_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_ramp_GRF.csv', delimiter=',')

subject_18_ramp=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_stair_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_stair_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_stair_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_stair_GRF.csv', delimiter=',')

subject_18_stair=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)

### subject 19 ###

#55.34
w19=68.04*9.81
h19=1.7

IMU_19= loadtxt('subject_19_treamill_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_treamill_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_treamill_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_treamill_GRF.csv', delimiter=',')

subject_19_treadmill=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_levelground_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_levelground_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_levelground_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_levelground_GRF.csv', delimiter=',')

subject_19_levelground=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_ramp_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_ramp_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_ramp_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_ramp_GRF.csv', delimiter=',')

subject_19_ramp=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_stair_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_stair_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_stair_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_stair_GRF.csv', delimiter=',')

subject_19_stair=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)

### subject 20 ###

#55.34
w20=68.04*9.81
h20=1.71

IMU_20= loadtxt('subject_20_treamill_IMU.csv', delimiter=',')
IK_20= loadtxt('subject_20_treamill_IK.csv', delimiter=',')
ID_20= loadtxt('subject_20_treamill_ID.csv', delimiter=',')
GRF_20= loadtxt('subject_20_treamill_GRF.csv', delimiter=',')

subject_20_treadmill=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_GRF.csv', delimiter=',')

# subject_20_levelground=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_GRF.csv', delimiter=',')

# subject_20_ramp=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_GRF.csv', delimiter=',')

# subject_20_stair=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)

### subject 21 ###

#55.34
w21=58.06*9.81
h21=1.57

IMU_21= loadtxt('subject_21_treamill_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_treamill_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_treamill_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_treamill_GRF.csv', delimiter=',')

subject_21_treadmill=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_levelground_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_levelground_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_levelground_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_levelground_GRF.csv', delimiter=',')

subject_21_levelground=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_ramp_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_ramp_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_ramp_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_ramp_GRF.csv', delimiter=',')

subject_21_ramp=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_stair_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_stair_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_stair_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_stair_GRF.csv', delimiter=',')

subject_21_stair=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)

### subject 23 ###

#55.34
w23=76.82*9.81
h23=1.8

IMU_23= loadtxt('subject_23_treamill_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_treamill_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_treamill_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_treamill_GRF.csv', delimiter=',')

subject_23_treadmill=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_levelground_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_levelground_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_levelground_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_levelground_GRF.csv', delimiter=',')

subject_23_levelground=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_ramp_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_ramp_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_ramp_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_ramp_GRF.csv', delimiter=',')

subject_23_ramp=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_stair_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_stair_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_stair_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_stair_GRF.csv', delimiter=',')

subject_23_stair=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)

### subject 24 ###

#55.34
w24=72.57*9.81
h24=1.73

IMU_24= loadtxt('subject_24_treamill_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_treamill_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_treamill_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_treamill_GRF.csv', delimiter=',')

subject_24_treadmill=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_levelground_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_levelground_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_levelground_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_levelground_GRF.csv', delimiter=',')

subject_24_levelground=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_ramp_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_ramp_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_ramp_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_ramp_GRF.csv', delimiter=',')

subject_24_ramp=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_stair_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_stair_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_stair_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_stair_GRF.csv', delimiter=',')

subject_24_stair=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)

### subject 25 ###

#55.34
w25=52.16*9.81
h25=1.63

IMU_25= loadtxt('subject_25_treamill_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_treamill_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_treamill_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_treamill_GRF.csv', delimiter=',')

subject_25_treadmill=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)

IMU_25= loadtxt('subject_25_levelground_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_levelground_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_levelground_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_levelground_GRF.csv', delimiter=',')

subject_25_levelground=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)


IMU_25= loadtxt('subject_25_ramp_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_ramp_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_ramp_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_ramp_GRF.csv', delimiter=',')

subject_25_ramp=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)


IMU_25= loadtxt('subject_25_stair_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_stair_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_stair_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_stair_GRF.csv', delimiter=',')

subject_25_stair=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)

### subject 27 ###

#55.34
w27=68.04*9.81
h27=1.7

IMU_27= loadtxt('subject_27_treamill_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_treamill_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_treamill_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_treamill_GRF.csv', delimiter=',')

subject_27_treadmill=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_levelground_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_levelground_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_levelground_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_levelground_GRF.csv', delimiter=',')

subject_27_levelground=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_ramp_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_ramp_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_ramp_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_ramp_GRF.csv', delimiter=',')

subject_27_ramp=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_stair_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_stair_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_stair_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_stair_GRF.csv', delimiter=',')

subject_27_stair=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)

### subject 28 ###

#55.34
w28=62.14*9.81
h28=1.69

IMU_28= loadtxt('subject_28_treamill_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_treamill_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_treamill_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_treamill_GRF.csv', delimiter=',')

subject_28_treadmill=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_levelground_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_levelground_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_levelground_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_levelground_GRF.csv', delimiter=',')

subject_28_levelground=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_ramp_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_ramp_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_ramp_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_ramp_GRF.csv', delimiter=',')

subject_28_ramp=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_stair_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_stair_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_stair_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_stair_GRF.csv', delimiter=',')

subject_28_stair=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)

### subject 30 ###

#55.34
w30=77.03*9.81
h30=1.77

IMU_30= loadtxt('subject_30_treamill_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_treamill_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_treamill_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_treamill_GRF.csv', delimiter=',')

subject_30_treadmill=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)



IMU_30= loadtxt('subject_30_levelground_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_levelground_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_levelground_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_levelground_GRF.csv', delimiter=',')

subject_30_levelground=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)


IMU_30= loadtxt('subject_30_ramp_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_ramp_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_ramp_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_ramp_GRF.csv', delimiter=',')

subject_30_ramp=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)


IMU_30= loadtxt('subject_30_stair_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_stair_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_stair_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_stair_GRF.csv', delimiter=',')

subject_30_stair=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)

gc.collect()


######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################



train_dataset_treadmill=np.concatenate((subject_7_treadmill,subject_8_treadmill,subject_9_treadmill,subject_10_treadmill,subject_11_treadmill,subject_12_treadmill,
                              subject_13_treadmill,subject_14_treadmill,subject_15_treadmill,\
                              subject_16_treadmill,subject_17_treadmill,subject_18_treadmill,subject_19_treadmill,subject_21_treadmill,\
                              subject_23_treadmill,subject_24_treadmill,subject_25_treadmill,subject_27_treadmill,subject_30_treadmill),axis=0)


train_dataset_levelground=np.concatenate((subject_7_levelground,subject_8_levelground,subject_9_levelground,subject_10_levelground,subject_11_levelground,subject_12_levelground,
                              subject_13_levelground,subject_14_levelground,subject_15_levelground,\
                              subject_16_levelground,subject_17_levelground,subject_18_levelground,subject_19_levelground,subject_21_levelground,\
                              subject_23_levelground,subject_24_levelground,subject_25_levelground,subject_27_levelground,subject_30_levelground),axis=0)


train_dataset_ramp=np.concatenate((subject_7_ramp,subject_8_ramp,subject_9_ramp,subject_10_ramp,subject_11_ramp,subject_12_ramp,
                              subject_13_ramp,subject_14_ramp,subject_15_ramp,\
                              subject_16_ramp,subject_17_ramp,subject_18_ramp,subject_19_ramp,subject_21_ramp,\
                              subject_23_ramp,subject_24_ramp,subject_25_ramp,subject_27_ramp,subject_30_ramp),axis=0)


train_dataset_stair=np.concatenate((subject_7_stair,subject_8_stair,subject_9_stair,subject_10_stair,subject_11_stair,subject_12_stair,
                              subject_13_stair,subject_14_stair,subject_15_stair,\
                              subject_16_stair,subject_17_stair,subject_18_stair,subject_19_stair,subject_21_stair,\
                              subject_23_stair,subject_24_stair,subject_25_stair,subject_27_stair,subject_30_stair),axis=0)


train_dataset=np.concatenate((train_dataset_treadmill,train_dataset_levelground,train_dataset_ramp,train_dataset_stair),axis=0)


test_dataset=np.concatenate((subject_28_treadmill,subject_28_levelground,subject_28_ramp,subject_28_stair),axis=0)


import os 
 
main_dir = "/home/sanzidpr/GRF_JM_SOTA_1/Subject28"
os.mkdir(main_dir) 

path='/home/sanzidpr/GRF_JM_SOTA_1/Subject28/'


subject='Subject_28'





#GRF_JM_SOTA_1


######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################



# Sensor 1- Sternum
# Sensor 2-Sacrum
# Sensor 3-R_thigh
# Sensor 4-L_thigh
# Sensor 5-R_shank
# Sensor 6-L_shank
# Sensor 7-R_dorsal
# Sensor 8-L_dorsal
 
 
# Train features #
train_1=train_dataset[:,1:7]
train_2=train_dataset[:,7:13]
train_3=train_dataset[:,13:19]
train_4=train_dataset[:,19:25]


from sklearn.preprocessing import StandardScaler


x_train=train_1
x_train=np.concatenate((train_3,train_2,train_1),axis=1)
scale= StandardScaler()
 
scaler = MinMaxScaler(feature_range=(0, 1))

train_X_1_1=x_train
 
 
 
 
# # Test features #
 
test_1=test_dataset[:,1:7]
test_2=test_dataset[:,7:13]
test_3=test_dataset[:,13:19]
test_4=test_dataset[:,19:25]



#    ### Extra features  ###
  
 
x_test=test_1

x_test=np.concatenate((test_3,test_2,test_1),axis=1)

 
test_X_1_1=x_test

 


  ### Label ###
    
f=0
 


y_1_1=train_dataset[:,(f+32):(f+33)]
y_1_2=train_dataset[:,(f+35):(f+37)]
y_1_3=train_dataset[:,(f+56):(f+57)]
y_1_4=train_dataset[:,(f+65):(f+66)]
y_1_5=train_dataset[:,(f+67):(f+68)]
y_1_6=train_dataset[:,(f+74):(f+77)]
y_1_7=train_dataset[:,(f+80):(f+83)]


# train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5,y_1_6,y_1_7),axis=1)

train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5,y_1_6),axis=1)

# train_y_1_1=y_1_4


y_2_1=test_dataset[:,(f+32):(f+33)]
y_2_2=test_dataset[:,(f+35):(f+37)]
y_2_3=test_dataset[:,(f+56):(f+57)]
y_2_4=test_dataset[:,(f+65):(f+66)]
y_2_5=test_dataset[:,(f+67):(f+68)]
y_2_6=test_dataset[:,(f+74):(f+77)]
y_2_7=test_dataset[:,(f+80):(f+83)]



 
# test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5,y_2_6,y_2_7),axis=1)

# test_y_1_1=y_2_4
test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5,y_2_6),axis=1)

train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)
test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)

train_dataset_1=pd.DataFrame(train_dataset_1)
test_dataset_1=pd.DataFrame(test_dataset_1)

train_dataset_1.dropna(axis=0,inplace=True)
test_dataset_1.dropna(axis=0,inplace=True)

train_dataset_1=np.array(train_dataset_1)
test_dataset_1=np.array(test_dataset_1)

train_dataset_sum = np. sum(train_dataset_1)
array_has_nan = np. isnan(train_dataset_sum)

print(array_has_nan)

print(train_dataset_1.shape)



train_X_1=train_dataset_1[:,0:18]
test_X_1=test_dataset_1[:,0:18]

train_y_1=train_dataset_1[:,18:24]
test_y_1=test_dataset_1[:,18:24]



L1=len(train_X_1)
L2=len(test_X_1)
# L3=len(validation_X_1)

print(L1+L2)
 
w=100

                   

 
 
a1=L1//w
b1=L1%w
 
a2=L2//w
b2=L2%w

# a3=L3//w
# b3=L3%w 
 
     #### Features ####
train_X_2=train_X_1[L1-w+b1:L1,:]
test_X_2=test_X_1[L2-w+b2:L2,:]
# validation_X_2=validation_X_1[L3-w+b3:L3,:]
 

    #### Output ####
 
train_y_2=train_y_1[L1-w+b1:L1,:]
test_y_2=test_y_1[L2-w+b2:L2,:]
# validation_y_2=validation_y_1[L3-w+b3:L3,:]


 
     #### Features ####
    
train_X=np.concatenate((train_X_1,train_X_2),axis=0)
test_X=np.concatenate((test_X_1,test_X_2),axis=0)
# validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)
 
 
    #### Output ####
    
train_y=np.concatenate((train_y_1,train_y_2),axis=0)
test_y=np.concatenate((test_y_1,test_y_2),axis=0)
# validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)

    
print(train_y.shape) 
    #### Reshaping ####
train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))
test_X = test_X.reshape((a2+1,w,test_X.shape[1]))


train_y_3_p= train_y.reshape((a1+1,w,6))
test_y= test_y.reshape((a2+1,w,6))
# Y_validation= validation_y.reshape((a3+1,w,6))

 

# train_X_1D=train_X_3
test_X_1D=test_X

train_X_3=train_X_3_p
train_y_3=train_y_3_p
# print(train_X_4.shape,train_y_3.shape)

train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)
#train_X_1D, X_validation_1D_ridge, train_y, Y_validation_ridge = train_test_split(train_X_1D_m,train_y_m, test_size=0.10, random_state=True)   [0:2668,:,:]

print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)

features=6

train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,3)
test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,3)
X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,3)
#X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)


print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)

import tensorflow as tf
# tensorflow import keras
from tensorflow.keras import layers

Bag_samples=train_X_2D.shape[0]
print(Bag_samples)

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()



  
  
from sklearn.model_selection import KFold
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
import pickle
from sklearn.linear_model import Ridge
from sklearn.utils import resample

"""# Loss Function"""

from keras import backend as K
def correlation_coefficient_loss(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l2
    return l

from keras import backend as K
def correlation_coefficient_loss_1(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1
    return l

from keras import backend as K
def correlation_coefficient_loss_joint(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1+l2
    return l
    
    
    
    

############################################################################################################################################################################################################################################################################################################################################################################################################################################################################

#####################################################################################################################################################################################################################################
###########################################################################################################################################################################################
###########################################################################################################################################################################################
def prediction_test(yhat_4,test_y):

    test_o=test_y.reshape((test_y.shape[0]*w,6))
    yhat=yhat_4.reshape((test_y.shape[0]*w,6))


    y_1_no=yhat[:,0]
    y_2_no=yhat[:,1]
    y_3_no=yhat[:,2]
    y_4_no=yhat[:,3]
    y_5_no=yhat[:,4]
    y_6_no=yhat[:,5]


    y_test_1=test_o[:,0]
    y_test_2=test_o[:,1]
    y_test_3=test_o[:,2]
    y_test_4=test_o[:,3]
    y_test_5=test_o[:,4]
    y_test_6=test_o[:,5]


    cutoff=6
    fs=200
    order=4

    nyq = 0.5 * fs
    ## filtering data ##
    def butter_lowpass_filter(data, cutoff, fs, order):
        normal_cutoff = cutoff / nyq
        # Get the filter coefficients 
        b, a = butter(order, normal_cutoff, btype='low', analog=False)
        y = filtfilt(b, a, data)
        return y



    y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
    y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
    y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
    y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
    y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
    y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)

    ###calculate RMSE

    rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
    rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
    rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
    rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
    rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
    rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



    print(rmse_1)
    print(rmse_2)
    print(rmse_3)
    print(rmse_4)
    print(rmse_5)
    print(rmse_6)



    p_1=np.corrcoef(y_1, y_test_1)[0, 1]
    p_2=np.corrcoef(y_2, y_test_2)[0, 1]
    p_3=np.corrcoef(y_3, y_test_3)[0, 1]
    p_4=np.corrcoef(y_4, y_test_4)[0, 1]
    p_5=np.corrcoef(y_5, y_test_5)[0, 1]
    p_6=np.corrcoef(y_6, y_test_6)[0, 1]

    print("\n") 
    print(p_1)
    print(p_2)
    print(p_3)
    print(p_4)
    print(p_5)
    print(p_6)


                ### Correlation ###
    p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




        #### Mean and standard deviation ####

    rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

        #### Mean and standard deviation ####
    m=statistics.mean(rmse)
    SD=statistics.stdev(rmse)
    print('Mean: %.3f' % m,'+/- %.3f' %SD)
    
    m_c=statistics.mean(p)
    SD_c=statistics.stdev(p)
    print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

    
    return rmse,p



########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################


 ### ANN (Features) ###

def feature_extractor(data):
  feat=[]
  feat_final=[]

  for i in range(18):
      signal=data[:,:,i]
      A_1=np.mean(signal,axis=1)
      A_2=np.sqrt(np.mean(signal ** 2,axis=1))
      A_3=np.min(signal,axis=1)   ## max- Statistical
      A_4=np.max(signal,axis=1)   ## min- Statistical
      A_5=np.mean(np.absolute(signal),axis=1)   ## mean- Statistical
      A_6=np.std(signal,axis=1)  ## standard Deviation- Statistical
      A_7=np.mean(np.abs(np.diff(signal,prepend=data[:,0:1,i],axis=1)),axis=1) ## Mean Absolute Difference
      A_8=np.mean(np.diff(signal,prepend=data[:,0:1,i],axis=1),axis=1) ## Mean Absolute Difference
      A_9=np.median(np.diff(signal,prepend=data[:,0:1,i],axis=1),axis=1) ## Mean  Difference
      A_10=np.median(np.abs(np.diff(signal,prepend=data[:,0:1,i],axis=1)),axis=1) ## Mean Absolute Difference
      A_11=np.percentile(signal, 75,axis=1) - np.percentile(signal, 25,axis=1)  # Interquartile Range-- Statistical
      A_12=scipy.stats.kurtosis(signal,axis=1)   ## Kurtosis--Statistical
      A_13=scipy.stats.skew(signal,axis=1)       ## Skewness--Statistical
      A_14=np.median(signal,axis=1) ## median- Statistical
      A_15=np.var(signal,axis=1)
      A_16=scipy.stats.median_absolute_deviation(signal,axis=1,scale=1)
      A_17=np.mean(np.abs(signal - np.mean(signal, axis=1).reshape(signal.shape[0],1)), axis=1)
      A_18=np.mean(np.diff(signal,prepend=data[:,0:1,i],axis=1),axis=1)
      dif=np.diff(data[:,:,i],prepend=data[:,0:1,i],axis=1)
      A_19=np.sum(np.absolute(dif),axis=1)  ### Waveform length
      A_20=np.sum(np.absolute(dif>0),axis=1)  ### Zero Crossing
      A_21=np.sum(np.absolute(np.diff(dif,prepend=data[:,0:1,i],axis=1))>0,axis=1)  ## Slope Sign Changes

      feat=np.vstack((A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10,A_11,A_12,A_13,A_14,A_15,A_16,A_17))
      # feat=np.vstack((A_18,A_19,A_20))
      feat_final.append((feat))
  feat_final=np.array(feat_final)    
  feat_final_1=feat_final.reshape([feat_final.shape[0]*feat_final.shape[1],feat_final.shape[-1]])
  feat_final_1=np.transpose(feat_final_1)

  return(feat_final_1)    
  
  
  
import scipy

X_train=feature_extractor(train_X_1D)
X_test=feature_extractor(test_X_1D)
X_validation=feature_extractor(X_validation_1D)


# 
w1=100

inputs_1D = tf.keras.layers.Input(shape=(X_train.shape[-1]))
inputs_1D_N=BatchNormalization()(inputs_1D)


#model_1=Dense(2048, activation='relu')(inputs_1D)
#model_1=Dropout(0.05)(model_1)
model_1=Dense(1024, activation='relu')(inputs_1D)
model_1=Dropout(0.05)(model_1)
model_1=Dense(512, activation='relu')(model_1)
model_1=Dropout(0.05)(model_1)
model_1=Dense(256, activation='relu')(model_1)
model_1=Dropout(0.05)(model_1)
model_1=Dense(128, activation='relu')(model_1)
model_1=Dropout(0.05)(model_1)
model_1=Dense(64, activation='relu')(model_1)
model_1=Dropout(0.05)(model_1)
model_1=Flatten()(model_1)


model_1=Dense(6*w1,bias_regularizer=l2(0.001), activation='linear')(model_1)
output=Reshape(target_shape=(w1,6))(model_1)

model = Model(inputs=inputs_1D, outputs=output)

opt = tf.keras.optimizers.Adam(learning_rate=3e-4)

model.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])

model.summary()

history=model.fit(X_train, train_y_5, epochs=40, batch_size=64, validation_data=(X_validation, Y_validation), verbose=2, shuffle=False)

model.save(path+'model_ANN(F).h5')

yhat_4= model.predict(X_test)
rmse,p= prediction_test(yhat_4, test_y)

ablation_1=np.hstack([rmse,p])

print(rmse)
print(p)  
  


##############################################################################################################################################################################################################################
##############################################################################################################################################################################################################################
##############################################################################################################################################################################################################################     

K.clear_session()
K.clear_session()


### TCN  ###
seed=7

w1=100

inputs_1D = tf.keras.layers.Input( shape=(w1,18))
inputs_1D_N=BatchNormalization()(inputs_1D)

model_1 = TCN(
    nb_filters=128,
    kernel_size=3,
    nb_stacks=1,
    dilations=(1, 2, 4, 8, 16),
    padding='causal',
    use_skip_connections=True,
    dropout_rate=0.05,
    return_sequences=True,
    activation='relu',
    kernel_initializer='he_normal',
    use_batch_norm=False,
    use_layer_norm=False,
    use_weight_norm=True
)(inputs_1D_N)



model_1=Dense(64, activation='relu')(model_1)
model_1=Dropout(0.1)(model_1)
model_1=Dense(32, activation='relu')(model_1)
model_1=Dropout(0.1)(model_1)
model_1=Flatten()(model_1)

model_1=Dense(6*w1,bias_regularizer=l2(0.001), activation='linear')(model_1)
output=Reshape(target_shape=(w1,6))(model_1)

model = Model(inputs=inputs_1D, outputs=output)

opt = tf.keras.optimizers.Adam(learning_rate=3e-4)

model.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])

model.summary()

history=model.fit(train_X_1D, train_y_5, epochs=40, batch_size=64, validation_data=(X_validation_1D, Y_validation), verbose=2, shuffle=False)
model.save(path+'model_TCN.h5')


yhat_4= model.predict(test_X_1D)
rmse,p= prediction_test(yhat_4, test_y)

ablation_2=np.hstack([rmse,p])

print(rmse)
print(p)



w1=100

inputs_1D = tf.keras.layers.Input( shape=(w1,18))
inputs_1D_N=BatchNormalization()(inputs_1D)


model_1=Dense(128, activation='relu')(inputs_1D_N)
model_1=Dropout(0.1)(model_1)
model_1=Dense(64, activation='relu')(model_1)
model_1=Dropout(0.1)(model_1)
model_1=Flatten()(model_1)

model_1=Dense(6*w1,bias_regularizer=l2(0.001), activation='linear')(model_1)
output=Reshape(target_shape=(w1,6))(model_1)

model = Model(inputs=inputs_1D, outputs=output)

opt = tf.keras.optimizers.Adam(learning_rate=3e-4)

model.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])

model.summary()

history=model.fit(train_X_1D, train_y_5, epochs=40, batch_size=64, validation_data=(X_validation_1D, Y_validation), verbose=2, shuffle=False)


model.save(path+'model_ANN.h5')

# yhat_main= model_1.predict([test_X_1D,test_X_2D])

yhat_4= model.predict(test_X_1D)
rmse,p= prediction_test(yhat_4, test_y)


ablation_3=np.hstack([rmse,p])


print(rmse)
print(p)



from numpy.ma.core import concatenate

w1=100

inputs_1D = tf.keras.layers.Input( shape=(w1,18))
inputs_1D_N=BatchNormalization()(inputs_1D)


# model_1=Bidirectional(GRU(128, return_sequences=True))(inputs_1D_N)
# model_1=Bidirectional(GRU(64, return_sequences=True))(model_1)

model_1=Bidirectional(LSTM(128, return_sequences=True))(inputs_1D_N)
model_1=Dropout(0.5)(model_1)
# model_2=tf.keras.layers.Concatenate()([model_1, inputs_1D_N])
model_1=Bidirectional(LSTM(64, return_sequences=True))(model_1)
model_1=Dropout(0.5)(model_1)
# model_1=tf.keras.layers.Concatenate()([model_1, model_2])
# model_1=LSTM(32, return_sequences=True)(model_2)

model_1=Dense(128, activation='relu')(model_1)
model_1=Dropout(0.25)(model_1)
model_1=Dense(64, activation='relu')(model_1)
model_1=Dropout(0.25)(model_1)
model_1=Flatten()(model_1)

model_1=Dense(6*w1,bias_regularizer=l2(0.001), activation='linear')(model_1)
output=Reshape(target_shape=(w1,6))(model_1)

model = Model(inputs=inputs_1D, outputs=output)

opt = tf.keras.optimizers.Adam(learning_rate=3e-4)

model.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])

model.summary()

history=model.fit(train_X_1D, train_y_5, epochs=25, batch_size=64, validation_data=(X_validation_1D, Y_validation), verbose=2, shuffle=False)


model.save(path+'model_LSTM.h5')

yhat_4= model.predict(test_X_1D)
rmse,p= prediction_test(yhat_4, test_y)


ablation_4=np.hstack([rmse,p])
print(rmse)
print(p)





All_result=np.vstack([ablation_1,ablation_2,ablation_3,ablation_4])

from numpy import savetxt

savetxt(path+subject+'_SOTA_results.csv', All_result, delimiter=',')




######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################





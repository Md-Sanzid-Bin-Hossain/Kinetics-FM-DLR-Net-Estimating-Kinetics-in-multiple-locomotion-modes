# -*- coding: utf-8 -*-
"""Future_Gait_DLR_Net_public_data_Stair_slope_Kinematics_kinetics_Public_dataset_kinematics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/116DLyfghavEPPeefrKabrJvohcuH2_xs
"""

# Let's import all packages that we may need:
import numpy
import tensorflow as tf
import statistics 
from numpy import loadtxt
import matplotlib.pyplot as plt
import pandas
import math
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import GRU,LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statistics import stdev 
import math

 
import numpy as np

from scipy.signal import butter,filtfilt
 
import sys 
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
import pandas
import matplotlib.pyplot as plt
 
## for Deep-learing:
import tensorflow.keras

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
to_categorical([0, 1, 2, 3], num_classes=4)
from tensorflow.keras.optimizers import SGD 
from tensorflow.keras.callbacks import EarlyStopping
# from tensorflow.keras.utils import np_utils
import itertools
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import TimeDistributed
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Bidirectional
#import constraint
 
from sklearn.model_selection import train_test_split
from tensorflow.keras.regularizers import l2
 
 
###  Library for attention layers 
 
import pandas as pd
#import pyarrow.parquet as pq # Used to read the data
import os 
import numpy as np
from tensorflow.keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes
from tensorflow.keras.models import Model
#from tqdm import tqdm # Processing time measurement
from sklearn.model_selection import train_test_split 
from tensorflow.keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class
from tensorflow.keras import optimizers # Allow us to access the Adam class to modify some parameters
from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model
from tensorflow.keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting
 
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K
from tensorflow.keras import initializers
from tensorflow.keras import regularizers
import statistics
import gc


 
### Early stopping 
 
from tensorflow.keras.callbacks import EarlyStopping

 

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.compat.v1.Session(config=config)

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())


#sess = tf.compat.v1.Session(config=tf.ConfigProto(log_device_placement=True))
#from google.colab import drive
#drive.mount('/content/drive',force_remount=True)

### subject 6 ###

#55.34

w6=74.84*9.81
h6=1.8

IMU_6= loadtxt('subject_6_treamill_IMU.csv', delimiter=',')
IK_6= loadtxt('subject_6_treamill_IK.csv', delimiter=',')
ID_6= loadtxt('subject_6_treamill_ID.csv', delimiter=',')
GRF_6= loadtxt('subject_6_treamill_GRF.csv', delimiter=',')

subject_6_treadmill=np.concatenate((IMU_6,IK_6,ID_6/(w6*h6),GRF_6/w6),axis=1)

### subject 7 ###

w7=55.34*9.81
h7=1.65

IMU_7= loadtxt('subject_7_treamill_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_treamill_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_treamill_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_treamill_GRF.csv', delimiter=',')

subject_7_treadmill=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_levelground_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_levelground_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_levelground_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_levelground_GRF.csv', delimiter=',')

subject_7_levelground=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_ramp_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_ramp_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_ramp_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_ramp_GRF.csv', delimiter=',')

subject_7_ramp=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_stair_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_stair_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_stair_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_stair_GRF.csv', delimiter=',')

subject_7_stair=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

### subject 8 ###

w8=72.57*9.81
h8=1.74

IMU_8= loadtxt('subject_8_treamill_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_treamill_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_treamill_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_treamill_GRF.csv', delimiter=',')

subject_8_treadmill=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_levelground_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_levelground_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_levelground_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_levelground_GRF.csv', delimiter=',')

subject_8_levelground=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_ramp_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_ramp_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_ramp_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_ramp_GRF.csv', delimiter=',')

subject_8_ramp=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_stair_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_stair_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_stair_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_stair_GRF.csv', delimiter=',')

subject_8_stair=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)

### subject 9 ###

w9=63.5*9.81
h9=1.63

IMU_9= loadtxt('subject_9_treamill_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_treamill_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_treamill_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_treamill_GRF.csv', delimiter=',')

subject_9_treadmill=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_levelground_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_levelground_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_levelground_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_levelground_GRF.csv', delimiter=',')

subject_9_levelground=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_ramp_IMU_1.csv', delimiter=',')
IK_9= loadtxt('subject_9_ramp_IK_1.csv', delimiter=',')
ID_9= loadtxt('subject_9_ramp_ID_1.csv', delimiter=',')
GRF_9= loadtxt('subject_9_ramp_GRF_1.csv', delimiter=',')

subject_9_ramp_1=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_ramp_IMU_2.csv', delimiter=',')
IK_9= loadtxt('subject_9_ramp_IK_2.csv', delimiter=',')
ID_9= loadtxt('subject_9_ramp_ID_2.csv', delimiter=',')
GRF_9= loadtxt('subject_9_ramp_GRF_2.csv', delimiter=',')

subject_9_ramp_2=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


subject_9_ramp=np.concatenate((subject_9_ramp_1,subject_9_ramp_2),axis=0)



IMU_9= loadtxt('subject_9_stair_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_stair_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_stair_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_stair_GRF.csv', delimiter=',')

subject_9_stair=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)

### subject 10 ###

w10=83.91*9.81
h10=1.75

IMU_10= loadtxt('subject_10_treamill_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_treamill_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_treamill_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_treamill_GRF.csv', delimiter=',')

subject_10_treadmill=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)


IMU_10= loadtxt('subject_10_levelground_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_levelground_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_levelground_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_levelground_GRF.csv', delimiter=',')

subject_10_levelground=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)


IMU_10= loadtxt('subject_10_ramp_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_ramp_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_ramp_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_ramp_GRF.csv', delimiter=',')

subject_10_ramp=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)

IMU_10= loadtxt('subject_10_stair_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_stair_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_stair_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_stair_GRF.csv', delimiter=',')

subject_10_stair=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)

### subject 11 ###

#55.34
w11=77.11*9.81
h11=1.75

IMU_11= loadtxt('subject_11_treamill_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_treamill_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_treamill_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_treamill_GRF.csv', delimiter=',')

subject_11_treadmill=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_levelground_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_levelground_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_levelground_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_levelground_GRF.csv', delimiter=',')

subject_11_levelground=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_ramp_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_ramp_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_ramp_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_ramp_GRF.csv', delimiter=',')

subject_11_ramp=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_stair_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_stair_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_stair_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_stair_GRF.csv', delimiter=',')

subject_11_stair=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)

### subject 12 ###

#55.34
w12=86.18*9.81
h12=1.74

IMU_12= loadtxt('subject_12_treamill_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_treamill_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_treamill_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_treamill_GRF.csv', delimiter=',')

subject_12_treadmill=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_levelground_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_levelground_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_levelground_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_levelground_GRF.csv', delimiter=',')

subject_12_levelground=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_ramp_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_ramp_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_ramp_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_ramp_GRF.csv', delimiter=',')

subject_12_ramp=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_stair_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_stair_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_stair_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_stair_GRF.csv', delimiter=',')

subject_12_stair=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)

### subject 13 ###

#55.34
w13=58.97*9.81
h13=1.73

IMU_13= loadtxt('subject_13_treamill_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_treamill_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_treamill_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_treamill_GRF.csv', delimiter=',')

subject_13_treadmill=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)



IMU_13= loadtxt('subject_13_levelground_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_levelground_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_levelground_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_levelground_GRF.csv', delimiter=',')

subject_13_levelground=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)


IMU_13= loadtxt('subject_13_ramp_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_ramp_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_ramp_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_ramp_GRF.csv', delimiter=',')

subject_13_ramp=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)


IMU_13= loadtxt('subject_13_stair_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_stair_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_stair_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_stair_GRF.csv', delimiter=',')

subject_13_stair=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)

### subject 14 ###

#55.34
w14=58.41*9.81
h14=1.52

IMU_14= loadtxt('subject_14_treamill_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_treamill_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_treamill_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_treamill_GRF.csv', delimiter=',')

subject_14_treadmill=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)



IMU_14= loadtxt('subject_14_levelground_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_levelground_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_levelground_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_levelground_GRF.csv', delimiter=',')

subject_14_levelground=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)


IMU_14= loadtxt('subject_14_ramp_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_ramp_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_ramp_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_ramp_GRF.csv', delimiter=',')

subject_14_ramp=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)


IMU_14= loadtxt('subject_14_stair_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_stair_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_stair_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_stair_GRF.csv', delimiter=',')

subject_14_stair=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)

### subject 15 ###

#55.34
w15=96.16*9.81
h15=1.78

IMU_15= loadtxt('subject_15_treamill_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_treamill_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_treamill_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_treamill_GRF.csv', delimiter=',')

subject_15_treadmill=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_levelground_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_levelground_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_levelground_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_levelground_GRF.csv', delimiter=',')

subject_15_levelground=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_ramp_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_ramp_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_ramp_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_ramp_GRF.csv', delimiter=',')

subject_15_ramp=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_stair_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_stair_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_stair_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_stair_GRF.csv', delimiter=',')

subject_15_stair=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)

### subject 16 ###

#55.34
w16=55.79*9.81
h16=1.65

IMU_16= loadtxt('subject_16_treamill_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_treamill_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_treamill_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_treamill_GRF.csv', delimiter=',')

subject_16_treadmill=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)

IMU_16= loadtxt('subject_16_levelground_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_levelground_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_levelground_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_levelground_GRF.csv', delimiter=',')

subject_16_levelground=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)


IMU_16= loadtxt('subject_16_ramp_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_ramp_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_ramp_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_ramp_GRF.csv', delimiter=',')

subject_16_ramp=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)


IMU_16= loadtxt('subject_16_stair_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_stair_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_stair_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_stair_GRF.csv', delimiter=',')

subject_16_stair=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)

### subject 17 ###

#55.34
w17=61.23*9.81
h17=1.68

IMU_17= loadtxt('subject_17_treamill_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_treamill_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_treamill_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_treamill_GRF.csv', delimiter=',')

subject_17_treadmill=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_levelground_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_levelground_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_levelground_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_levelground_GRF.csv', delimiter=',')

subject_17_levelground=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_ramp_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_ramp_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_ramp_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_ramp_GRF.csv', delimiter=',')

subject_17_ramp=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_stair_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_stair_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_stair_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_stair_GRF.csv', delimiter=',')

subject_17_stair=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)

### subject 18 ###

#55.34
w18=60.13*9.81
h18=1.8

IMU_18= loadtxt('subject_18_treamill_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_treamill_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_treamill_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_treamill_GRF.csv', delimiter=',')

subject_18_treadmill=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_levelground_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_levelground_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_levelground_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_levelground_GRF.csv', delimiter=',')

subject_18_levelground=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_ramp_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_ramp_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_ramp_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_ramp_GRF.csv', delimiter=',')

subject_18_ramp=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_stair_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_stair_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_stair_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_stair_GRF.csv', delimiter=',')

subject_18_stair=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)

### subject 19 ###

#55.34
w19=68.04*9.81
h19=1.7

IMU_19= loadtxt('subject_19_treamill_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_treamill_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_treamill_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_treamill_GRF.csv', delimiter=',')

subject_19_treadmill=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_levelground_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_levelground_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_levelground_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_levelground_GRF.csv', delimiter=',')

subject_19_levelground=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_ramp_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_ramp_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_ramp_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_ramp_GRF.csv', delimiter=',')

subject_19_ramp=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_stair_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_stair_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_stair_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_stair_GRF.csv', delimiter=',')

subject_19_stair=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)

### subject 20 ###

#55.34
w20=68.04*9.81
h20=1.71

IMU_20= loadtxt('subject_20_treamill_IMU.csv', delimiter=',')
IK_20= loadtxt('subject_20_treamill_IK.csv', delimiter=',')
ID_20= loadtxt('subject_20_treamill_ID.csv', delimiter=',')
GRF_20= loadtxt('subject_20_treamill_GRF.csv', delimiter=',')

subject_20_treadmill=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_GRF.csv', delimiter=',')

# subject_20_levelground=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_GRF.csv', delimiter=',')

# subject_20_ramp=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_GRF.csv', delimiter=',')

# subject_20_stair=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)

### subject 21 ###

#55.34
w21=58.06*9.81
h21=1.57

IMU_21= loadtxt('subject_21_treamill_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_treamill_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_treamill_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_treamill_GRF.csv', delimiter=',')

subject_21_treadmill=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_levelground_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_levelground_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_levelground_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_levelground_GRF.csv', delimiter=',')

subject_21_levelground=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_ramp_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_ramp_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_ramp_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_ramp_GRF.csv', delimiter=',')

subject_21_ramp=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_stair_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_stair_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_stair_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_stair_GRF.csv', delimiter=',')

subject_21_stair=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)

### subject 23 ###

#55.34
w23=76.82*9.81
h23=1.8

IMU_23= loadtxt('subject_23_treamill_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_treamill_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_treamill_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_treamill_GRF.csv', delimiter=',')

subject_23_treadmill=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_levelground_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_levelground_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_levelground_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_levelground_GRF.csv', delimiter=',')

subject_23_levelground=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_ramp_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_ramp_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_ramp_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_ramp_GRF.csv', delimiter=',')

subject_23_ramp=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_stair_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_stair_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_stair_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_stair_GRF.csv', delimiter=',')

subject_23_stair=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)

### subject 24 ###

#55.34
w24=72.57*9.81
h24=1.73

IMU_24= loadtxt('subject_24_treamill_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_treamill_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_treamill_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_treamill_GRF.csv', delimiter=',')

subject_24_treadmill=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_levelground_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_levelground_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_levelground_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_levelground_GRF.csv', delimiter=',')

subject_24_levelground=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_ramp_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_ramp_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_ramp_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_ramp_GRF.csv', delimiter=',')

subject_24_ramp=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_stair_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_stair_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_stair_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_stair_GRF.csv', delimiter=',')

subject_24_stair=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)

### subject 25 ###

#55.34
w25=52.16*9.81
h25=1.63

IMU_25= loadtxt('subject_25_treamill_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_treamill_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_treamill_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_treamill_GRF.csv', delimiter=',')

subject_25_treadmill=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)

IMU_25= loadtxt('subject_25_levelground_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_levelground_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_levelground_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_levelground_GRF.csv', delimiter=',')

subject_25_levelground=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)


IMU_25= loadtxt('subject_25_ramp_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_ramp_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_ramp_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_ramp_GRF.csv', delimiter=',')

subject_25_ramp=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)


IMU_25= loadtxt('subject_25_stair_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_stair_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_stair_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_stair_GRF.csv', delimiter=',')

subject_25_stair=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)

### subject 27 ###

#55.34
w27=68.04*9.81
h27=1.7

IMU_27= loadtxt('subject_27_treamill_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_treamill_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_treamill_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_treamill_GRF.csv', delimiter=',')

subject_27_treadmill=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_levelground_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_levelground_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_levelground_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_levelground_GRF.csv', delimiter=',')

subject_27_levelground=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_ramp_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_ramp_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_ramp_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_ramp_GRF.csv', delimiter=',')

subject_27_ramp=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_stair_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_stair_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_stair_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_stair_GRF.csv', delimiter=',')

subject_27_stair=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)

### subject 28 ###

#55.34
w28=62.14*9.81
h28=1.69

IMU_28= loadtxt('subject_28_treamill_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_treamill_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_treamill_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_treamill_GRF.csv', delimiter=',')

subject_28_treadmill=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_levelground_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_levelground_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_levelground_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_levelground_GRF.csv', delimiter=',')

subject_28_levelground=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_ramp_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_ramp_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_ramp_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_ramp_GRF.csv', delimiter=',')

subject_28_ramp=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_stair_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_stair_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_stair_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_stair_GRF.csv', delimiter=',')

subject_28_stair=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)

### subject 30 ###

#55.34
w30=77.03*9.81
h30=1.77

IMU_30= loadtxt('subject_30_treamill_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_treamill_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_treamill_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_treamill_GRF.csv', delimiter=',')

subject_30_treadmill=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)



IMU_30= loadtxt('subject_30_levelground_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_levelground_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_levelground_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_levelground_GRF.csv', delimiter=',')

subject_30_levelground=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)


IMU_30= loadtxt('subject_30_ramp_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_ramp_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_ramp_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_ramp_GRF.csv', delimiter=',')

subject_30_ramp=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)


IMU_30= loadtxt('subject_30_stair_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_stair_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_stair_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_stair_GRF.csv', delimiter=',')

subject_30_stair=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)

gc.collect()


######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################


train_dataset_treadmill=np.concatenate((subject_7_treadmill,subject_8_treadmill,subject_9_treadmill,subject_10_treadmill,
                              subject_11_treadmill,subject_12_treadmill,subject_13_treadmill,subject_14_treadmill,\
                              subject_15_treadmill,subject_16_treadmill,subject_17_treadmill,subject_18_treadmill,subject_19_treadmill,\
                              subject_21_treadmill,subject_23_treadmill,subject_24_treadmill,subject_25_treadmill,subject_28_treadmill,subject_30_treadmill),axis=0)


train_dataset_levelground=np.concatenate((subject_7_levelground,subject_8_levelground,subject_9_levelground,subject_10_levelground,
                              subject_11_levelground,subject_12_levelground,subject_13_levelground,subject_14_levelground,\
                              subject_15_levelground,subject_16_levelground,subject_17_levelground,subject_18_levelground,subject_19_levelground,\
                              subject_21_levelground,subject_23_levelground,subject_24_levelground,subject_25_levelground,subject_28_levelground,subject_30_levelground),axis=0)


train_dataset_ramp=np.concatenate((subject_7_ramp,subject_8_ramp,subject_9_ramp,subject_10_ramp,
                              subject_11_ramp,subject_12_ramp,subject_13_ramp,subject_14_ramp,\
                              subject_15_ramp,subject_16_ramp,subject_17_ramp,subject_18_ramp,subject_19_ramp,\
                              subject_21_ramp,subject_23_ramp,subject_24_ramp,subject_25_ramp,subject_28_ramp,subject_30_ramp),axis=0)


train_dataset_stair=np.concatenate((subject_7_stair,subject_8_stair,subject_9_stair,subject_10_stair,
                              subject_11_stair,subject_12_stair,subject_13_stair,subject_14_stair,\
                              subject_15_stair,subject_16_stair,subject_17_stair,subject_18_stair,subject_19_stair,\
                              subject_21_stair,subject_23_stair,subject_24_stair,subject_25_stair,subject_28_stair,subject_30_stair),axis=0)


train_dataset=np.concatenate((train_dataset_treadmill,train_dataset_levelground,train_dataset_ramp,train_dataset_stair),axis=0)


test_dataset=np.concatenate((subject_27_treadmill,subject_27_levelground,subject_27_ramp,subject_27_stair),axis=0)

import os 
 
main_dir = "/home/sanzidpr/Kinetics-FM-Bag-Net/Subject27"
os.mkdir(main_dir) 

path='/home/sanzidpr/Kinetics-FM-Bag-Net/Subject27/'

######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################
print(train_dataset.shape)

# Sensor 1- Sternum
# Sensor 2-Sacrum
# Sensor 3-R_thigh
# Sensor 4-L_thigh
# Sensor 5-R_shank
# Sensor 6-L_shank
# Sensor 7-R_dorsal
# Sensor 8-L_dorsal
 
 
# Train features #
train_1=train_dataset[:,1:7]
train_2=train_dataset[:,7:13]
train_3=train_dataset[:,13:19]
train_4=train_dataset[:,19:25]


from sklearn.preprocessing import StandardScaler


x_train=train_1
x_train=np.concatenate((train_3,train_2,train_1),axis=1)
scale= StandardScaler()
 
scaler = MinMaxScaler(feature_range=(0, 1))

train_X_1_1=x_train
 
 
 
 
# # Test features #
 
test_1=test_dataset[:,1:7]
test_2=test_dataset[:,7:13]
test_3=test_dataset[:,13:19]
test_4=test_dataset[:,19:25]



#    ### Extra features  ###
  
 
x_test=test_1

x_test=np.concatenate((test_3,test_2,test_1),axis=1)

 
test_X_1_1=x_test

 


  ### Label ###
    
f=0
 


y_1_1=train_dataset[:,(f+32):(f+33)]
y_1_2=train_dataset[:,(f+35):(f+37)]
y_1_3=train_dataset[:,(f+56):(f+57)]
y_1_4=train_dataset[:,(f+65):(f+66)]
y_1_5=train_dataset[:,(f+67):(f+68)]
y_1_6=train_dataset[:,(f+74):(f+77)]
y_1_7=train_dataset[:,(f+80):(f+83)]


# train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5,y_1_6,y_1_7),axis=1)

train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5,y_1_6),axis=1)

# train_y_1_1=y_1_4


y_2_1=test_dataset[:,(f+32):(f+33)]
y_2_2=test_dataset[:,(f+35):(f+37)]
y_2_3=test_dataset[:,(f+56):(f+57)]
y_2_4=test_dataset[:,(f+65):(f+66)]
y_2_5=test_dataset[:,(f+67):(f+68)]
y_2_6=test_dataset[:,(f+74):(f+77)]
y_2_7=test_dataset[:,(f+80):(f+83)]



 
# test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5,y_2_6,y_2_7),axis=1)

# test_y_1_1=y_2_4
test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5,y_2_6),axis=1)

train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)
test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)

train_dataset_1=pd.DataFrame(train_dataset_1)
test_dataset_1=pd.DataFrame(test_dataset_1)

train_dataset_1.dropna(axis=0,inplace=True)
test_dataset_1.dropna(axis=0,inplace=True)

train_dataset_1=np.array(train_dataset_1)
test_dataset_1=np.array(test_dataset_1)

train_dataset_sum = np. sum(train_dataset_1)
array_has_nan = np. isnan(train_dataset_sum)

print(array_has_nan)

print(train_dataset_1.shape)



train_X_1=train_dataset_1[:,0:18]
test_X_1=test_dataset_1[:,0:18]

train_y_1=train_dataset_1[:,18:24]
test_y_1=test_dataset_1[:,18:24]



L1=len(train_X_1)
L2=len(test_X_1)
# L3=len(validation_X_1)

print(L1+L2)
 
w=100

                   

 
 
a1=L1//w
b1=L1%w
 
a2=L2//w
b2=L2%w

# a3=L3//w
# b3=L3%w 
 
     #### Features ####
train_X_2=train_X_1[L1-w+b1:L1,:]
test_X_2=test_X_1[L2-w+b2:L2,:]
# validation_X_2=validation_X_1[L3-w+b3:L3,:]
 

    #### Output ####
 
train_y_2=train_y_1[L1-w+b1:L1,:]
test_y_2=test_y_1[L2-w+b2:L2,:]
# validation_y_2=validation_y_1[L3-w+b3:L3,:]


 
     #### Features ####
    
train_X=np.concatenate((train_X_1,train_X_2),axis=0)
test_X=np.concatenate((test_X_1,test_X_2),axis=0)
# validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)
 
 
    #### Output ####
    
train_y=np.concatenate((train_y_1,train_y_2),axis=0)
test_y=np.concatenate((test_y_1,test_y_2),axis=0)
# validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)

    
print(train_y.shape) 
    #### Reshaping ####
train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))
test_X = test_X.reshape((a2+1,w,test_X.shape[1]))


train_y_3_p= train_y.reshape((a1+1,w,6))
test_y= test_y.reshape((a2+1,w,6))
# Y_validation= validation_y.reshape((a3+1,w,6))

 

# train_X_1D=train_X_3
test_X_1D=test_X

train_X_3=train_X_3_p
train_y_3=train_y_3_p
# print(train_X_4.shape,train_y_3.shape)

train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)
#train_X_1D, X_validation_1D_ridge, train_y, Y_validation_ridge = train_test_split(train_X_1D_m,train_y_m, test_size=0.10, random_state=True)   [0:2668,:,:]

print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)

features=6

train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,3)
test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,3)
X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,3)
#X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)


print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)

import tensorflow as tf
# tensorflow import keras
from tensorflow.keras import layers

Bag_samples=train_X_2D.shape[0]
print(Bag_samples)

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

"""# Function of Base Models"""

##################### 1. GRU-Net  #################################################


def GRU_Net(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  

  return (output_GRU)

  
  
def GRU_Net_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  return (output_GRU)
  
  

#############################################################################################################################################
########################  2. Conv2D-Net  ####################################################################################################
  
  
def Conv2D_Net(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  #X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  return (output_C2)

  
  
def Conv2D_Net_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  #X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  return (output_C2)  
  
###################################################################################################################################################################
#########################################################  3. Conv1D-Net  ######################################################################################################

def Conv1D_Net(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  #X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  #CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  

  return (output_C1)

  
  
def Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  #X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  #CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  return (output_C1)  
  
####################################################################################################################################################################################  

#############################################################################################################################################
########################  4. GRU-Conv2D-Net  ####################################################################################################
  
  
def GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  return (output_C2)

  
  
def GRU_Conv2D_Net_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  return (output_C2)  
  
###################################################################################################################################################################################  
#########################################################  5. GRU-Conv1D-Net  ######################################################################################################

def GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  #X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  

  return (output_C1)

  
  
def GRU_Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  #X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  return (output_C1)  
  
####################################################################################################################################################################################    

###################################################################################################################################################################################  
#########################################################  6. Conv2D-Conv1D-Net  ######################################################################################################

def Conv2D_Conv1D_Net(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  #X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,X])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  

  return (output_C1)

  
  
def Conv2D_Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  #X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,X])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  return (output_C1)  
  
################################################################################################################################################################################################
#################################################### 7. Kinetics-Sub-Net-1 #######################################################################################################################  
 
 
def Kinetics_Sub_Net_1(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  output = Average()([output_GRU,output_C2])
  

  return (output_C2,output_GRU,output)

  
  
def Kinetics_Sub_Net_1_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  output = Average()([output_GRU,output_C2])
  
  return (output_C2,output_GRU,output)
    
#########################################################################################################################################################################  
################################################################################################################################################################################################
#################################################### 8. Kinetics-Sub-Net-2 #######################################################################################################################  
 
 
def Kinetics_Sub_Net_2(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  output = Average()([output_GRU,output_C1])
  

  return (output_C1,output_GRU,output)

  
  
def Kinetics_Sub_Net_2_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  output = Average()([output_GRU,output_C1])
  
  return (output_C1,output_GRU,output)
    
######################################################################################################################################################################### 

################################################################################################################################################################################################
#################################################### 9. Kinetics-Sub-Net-2 #######################################################################################################################  
 
 
def Kinetics_Sub_Net_3(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  output = Average()([output_C2,output_C1])
  

  return (output_C1,output_C2,output)

  
  
def Kinetics_Sub_Net_3_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  output = Average()([output_C2,output_C1])
  
  return (output_C1,output_C2,output)
    
#########################################################################################################################################################################  
####################################################   10. Kinetics-Net   ####################################################################

def Kinetics_Net(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  output = Average()([output_GRU,output_C2,output_C1])
  

  return (output_C2,output_GRU,output_C1,output)

  
  
def Kinetics_Net_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
   
  output = Average()([output_GRU,output_C2,output_C1])
  
  return (output_C2,output_GRU,output_C1,output)
  
##########################################################################################################################################################################################  
##########################################################################################################################################################################################

#########################################################################################################################################################################  
####################################################   10. Kinetics-FM-Net   ####################################################################

def Kinetics_FM_Net(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  output_GRU_1=Dense(128,activation='relu')(output_GRU)
  output_GRU_1=Dense(6,activation='sigmoid')(output_GRU_1)
  #output_GRU_1=Dense(1,activation='sigmoid')(output_GRU_1)
  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])
  
  output_C2_1=Dense(128,activation='relu')(output_C2)
  output_C2_1=Dense(6,activation='sigmoid')(output_C2_1)
  #output_C2_1=Dense(1,activation='sigmoid')(output_C2_1)
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])

  output_C1_1=Dense(128,activation='relu')(output_C1)
  output_C1_1=Dense(6,activation='sigmoid')(output_C1_1)
  #output_C1_1=Dense(1,activation='sigmoid')(output_C1_1)
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])
  

  weight=output_GRU_1+output_C2_1+output_C1_1
  
  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])
  

  output = output_GRU_2+output_C1_2+output_C2_2
  
  #output = Average()([output_GRU,output_C2,output_C1])
  

  return (output_C2,output_GRU,output_C1,output)

  
  
def Kinetics_FM_Net_pcc(inputs_1D_N,inputs_2D_N):

#   model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#   model_1=Dropout(0.35)(model_1)
#   model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#   model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.5)(model_1)
#   model_1=Dense(64, activation='relu')(model_1)
#   model_1=Dropout(0.3)(model_1)
#   model_1=Dense(32,activation='relu')(model_1)
#   Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (3, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])
  
  Feat_1=concatenate([model_1,X])
  Feat_2=concatenate([model_1,CNN])
  Feat_3=concatenate([X,CNN])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,6))(output_GRU)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,6))(output_C2)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,6))(output_C1)
  
  
  output_GRU_1=Dense(128,activation='relu')(output_GRU)
  output_GRU_1=Dropout(0.4)(output_GRU_1)
  #output_GRU_1=Dense(64,activation='relu')(output_GRU_1)
  output_GRU_1=Dense(6,activation='sigmoid')(output_GRU_1)
  #output_GRU_1=Dense(1,activation='sigmoid')(output_GRU_1)
  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])
  
  output_C2_1=Dense(128,activation='relu')(output_C2)
  output_C2_1=Dropout(0.4)(output_C2_1)
  #output_C2_1=Dense(64,activation='relu')(output_C2_1)
  output_C2_1=Dense(6,activation='sigmoid')(output_C2_1)
  #output_C2_1=Dense(1,activation='sigmoid')(output_C2_1)
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])

  output_C1_1=Dense(128,activation='relu')(output_C1)
  output_C1_1=Dropout(0.4)(output_C1_1)
  #output_C1_1=Dense(64,activation='relu')(output_C1_1)
  output_C1_1=Dense(6,activation='sigmoid')(output_C1_1)
  #output_C1_1=Dense(1,activation='sigmoid')(output_C1_1)
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])
  

  weight=output_GRU_1+output_C2_1+output_C1_1
  
  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])
  
  output = output_GRU_2+output_C1_2+output_C2_2
  
  #output = Average()([output_GRU,output_C2,output_C1])
  
  return (output_C2,output_GRU,output_C1,output)
  
  
##########################################################################################################################################################################################  
##########################################################################################################################################################################################    

  
  
  
from sklearn.model_selection import KFold
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
import pickle
from sklearn.linear_model import Ridge
from sklearn.utils import resample

"""# Loss Function"""

from keras import backend as K
def correlation_coefficient_loss(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l2
    return l

from keras import backend as K
def correlation_coefficient_loss_1(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1
    return l

from keras import backend as K
def correlation_coefficient_loss_joint(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1+l2
    return l
    
    
    
    

############################################################################################################################################################################################################################################################################################################################################################################################################################################################################

"""# GRU-Net, GRU-DLR-Net, GRU-JL-Net"""

### GRU-NET ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Net(inputs_1D_N,inputs_2D_N)

model_1 = tf.keras.Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()

model_1.save(path+'model_GRU.h5')
print('>Saved %s' % path)

############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


gc.collect()

#### All 16 angles prediction  ####
yhat_4 = model_1.predict([test_X_1D,test_X_2D])

# yhat_4=(yhat_1+yhat_2+yhat_3)/3

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))


# test_o=test_o[0:s-14,:]
# yhat=yhat[14:s,:]



y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]





#print(y_1.shape,y_test_1.shape)



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)

            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_GRU=rmse
PCC_GRU=p

gc.collect()
gc.collect()


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


### GRU-NET ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Net_pcc(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()


model_2.save(path+'model_GRU_PCC.h5')



############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


yhat_4 = model_2.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))



y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]

cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)

#### All 16 angles prediction  ####


yhat_4= model_2.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))


y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100





print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])



    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_GRU_DLR=rmse
PCC_GRU_DLR=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

############################################################################################################################################################################################################################################################################################################################################################################################################################################################################

### GRU-NET ###
  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Net(inputs_1D_N,inputs_2D_N)
model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()


model_3.save(path+'model_GRU_JL.h5')


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


#### All 16 angles prediction  ####

yhat_4= model_3.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))


y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_GRU_JL=rmse
PCC_GRU_JL=p



ablation_1=np.hstack([RMSE_GRU,PCC_GRU])
ablation_2=np.hstack([RMSE_GRU_DLR,PCC_GRU_DLR])
ablation_3=np.hstack([RMSE_GRU_JL,PCC_GRU_JL])

GRU_result=np.vstack([ablation_1,ablation_2,ablation_3])

from numpy import savetxt




##############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


"""# Conv2D-Net, Conv2D-DLR-Net, Conv2D-JL-Net"""

### Conv2D-NET ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=Conv2D_Net(inputs_1D_N,inputs_2D_N)

model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()


model_1.save(path+'model_Conv2D.h5')


############################################################################################################################################################################################################################################################################################################################################################################################################################################################################


gc.collect()

#### All 16 angles prediction  ####

yhat_4 = model_1.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))



y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100






print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Conv2D=rmse
PCC_Conv2D=p

gc.collect()
gc.collect()

### GRU-NET ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=Conv2D_Net_pcc(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()


model_2.save(path+'model_Conv2D_PCC.h5')


gc.collect()

#### All 16 angles prediction  ####

yhat_4 = model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))



y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6






###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100






print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)


yhat_4= model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))





y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)




y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)


###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100

print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Conv2D_DLR=rmse
PCC_Conv2D_DLR=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

### Conv2D-NET ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=Conv2D_Net(inputs_1D_N,inputs_2D_N)

model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()


model_3.save(path+'model_Conv2D_JL.h5')

yhat_4= model_3.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Conv2D_JL=rmse
PCC_Conv2D_JL=p

ablation_4=np.hstack([RMSE_Conv2D,PCC_Conv2D])
ablation_5=np.hstack([RMSE_Conv2D_DLR,PCC_Conv2D_DLR])
ablation_6=np.hstack([RMSE_Conv2D_JL,PCC_Conv2D_JL])

Conv2D_result=np.vstack([ablation_4,ablation_5,ablation_6])



"""# Conv1D-Net, Conv1D-DLR-Net, Conv1D-JL-Net

"""

### Conv1D-NET ###
  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=Conv1D_Net(inputs_1D_N,inputs_2D_N)

model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()


model_1.save(path+'model_Conv1D.h5')


gc.collect()

#### All 16 angles prediction  ####
yhat_4 = model_1.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Conv1D=rmse
PCC_Conv1D=p

gc.collect()
gc.collect()

### Conv1D-NET ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

model_2.save(path+'model_Conv1D_PCC.h5')


gc.collect()


yhat_4 = model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))



y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6


###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)


yhat_4= model_2.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)




y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)





###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Conv1D_DLR=rmse
PCC_Conv1D_DLR=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

### Conv1D-NET ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=Conv1D_Net(inputs_1D_N,inputs_2D_N)

model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()


model_3.save(path+'model_Conv1D_JL.h5')



yhat_4= model_3.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]





y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)

p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Conv1D_JL=rmse
PCC_Conv1D_JL=p

ablation_7=np.hstack([RMSE_Conv1D,PCC_Conv1D])
ablation_8=np.hstack([RMSE_Conv1D_DLR,PCC_Conv1D_DLR])
ablation_9=np.hstack([RMSE_Conv1D_JL,PCC_Conv1D_JL])

Conv1D_result=np.vstack([ablation_7,ablation_8,ablation_9])




######################################################################################################################################################################################################


#############   GRU-Conv2D-Net	GRU-Conv2D-DLR-Net	GRU-Conv2D-JL-Net    ##################



### GRU-Conv2D-Net ###
  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N)

model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()


model_1.save(path+'model_GRU_Conv2D.h5')


gc.collect()

#### All 16 angles prediction  ####
yhat_4 = model_1.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_GRU_Conv2D=rmse
PCC_GRU_Conv2D=p

gc.collect()
gc.collect()

### GRU-Conv2D-DLR-Net ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Conv2D_Net_pcc(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

model_2.save(path+'model_GRU_Conv2D_PCC.h5')


gc.collect()


yhat_4 = model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))



y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6


###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)


yhat_4= model_2.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)




y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)





###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_GRU_Conv2D_DLR=rmse
PCC_GRU_Conv2D_DLR=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

#################   GRU-Conv2D-JL-Net    #####################


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N)

model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()

model_3.save(path+'model_GRU_Conv2D_JL.h5')



yhat_4= model_3.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]





y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)

p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_GRU_Conv2D_JL=rmse
PCC_GRU_Conv2D_JL=p

ablation_10=np.hstack([RMSE_GRU_Conv2D,PCC_GRU_Conv2D])
ablation_11=np.hstack([RMSE_GRU_Conv2D_DLR,PCC_GRU_Conv2D_DLR])
ablation_12=np.hstack([RMSE_GRU_Conv2D_JL,PCC_GRU_Conv2D_JL])

GRU_Conv2D_result=np.vstack([ablation_10,ablation_11,ablation_12])


#################################################################################################################################################################################################################



######################################################################################################################################################################################################


#############   GRU-Conv1D-Net	GRU-Conv1D-DLR-Net	GRU-Conv1D-JL-Net    ##################



### GRU-Conv1D-Net ###
  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N)

model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()


model_1.save(path+'model_GRU_Conv1D.h5')


gc.collect()

#### All 16 angles prediction  ####
yhat_4 = model_1.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_GRU_Conv1D=rmse
PCC_GRU_Conv1D=p


gc.collect()
gc.collect()

### GRU-Conv1D-DLR-Net ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

model_2.save(path+'model_GRU_Conv1D_PCC.h5')


gc.collect()


yhat_4 = model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))



y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6


###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)


yhat_4= model_2.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)




y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)





###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_GRU_Conv1D_DLR=rmse
PCC_GRU_Conv1D_DLR=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

#################   GRU-Conv1D-JL-Net    #####################


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N)

model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()

model_3.save(path+'model_GRU_Conv1D_JL.h5')



yhat_4= model_3.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]





y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)

p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_GRU_Conv1D_JL=rmse
PCC_GRU_Conv1D_JL=p

ablation_13=np.hstack([RMSE_GRU_Conv1D,PCC_GRU_Conv1D])
ablation_14=np.hstack([RMSE_GRU_Conv1D_DLR,PCC_GRU_Conv1D_DLR])
ablation_15=np.hstack([RMSE_GRU_Conv1D_JL,PCC_GRU_Conv1D_JL])

GRU_Conv1D_result=np.vstack([ablation_13,ablation_14,ablation_15])


##########################################################################################################################################################################################################################
##########################################################################################################################################################################################################################

######################################################################################################################################################################################################


#############   Conv2D-Conv1D-Net	Conv2D-Conv1D-DLR-Net	Conv2D-Conv1D-JL-Net    ##################



### Conv2D-Conv1D-Net ###
  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=Conv2D_Conv1D_Net(inputs_1D_N,inputs_2D_N)

model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()


model_1.save(path+'model_Conv2D_Conv1D.h5')


gc.collect()

#### All 16 angles prediction  ####
yhat_4 = model_1.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Conv2D_Conv1D=rmse
PCC_Conv2D_Conv1D=p


gc.collect()
gc.collect()

### Conv2D-Conv1D-DLR-Net ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=Conv2D_Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

model_2.save(path+'model_Conv2D_Conv1D_PCC.h5')


gc.collect()


yhat_4 = model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))



y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6


###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)


yhat_4= model_2.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)




y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)





###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Conv2D_Conv1D_DLR=rmse
PCC_Conv2D_Conv1D_DLR=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

#################   Conv2D-Conv1D-JL-Net    #####################


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)

output=GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N)

model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)

model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)


# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()

model_3.save(path+'model_Conv2D_Conv1D_JL.h5')



yhat_4= model_3.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]





y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)

p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Conv2D_Conv1D_JL=rmse
PCC_Conv2D_Conv1D_JL=p

ablation_16=np.hstack([RMSE_Conv2D_Conv1D,PCC_Conv2D_Conv1D])
ablation_17=np.hstack([RMSE_Conv2D_Conv1D_DLR,PCC_Conv2D_Conv1D_DLR])
ablation_18=np.hstack([RMSE_Conv2D_Conv1D_JL,PCC_Conv2D_Conv1D_JL])

Conv2D_Conv1D_result=np.vstack([ablation_16,ablation_17,ablation_18])


##########################################################################################################################################################################################################################
##########################################################################################################################################################################################################################


""# Kinetics-Sub-Net-1,  Kinetics-Sub-DLR-Net-1,  Kinetics-Sub-JL-Net-1"

### Kinetics-Sub-Net-1 ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Kinetics_Sub_Net_1(inputs_1D_N,inputs_2D_N)

model_1= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])


model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()

gc.collect()


model_1.save(path+'model_Kinetics_Sub_1.h5')

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_4] = model_1.predict([test_X_1D,test_X_2D])





s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100




print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Kinetics_Sub_Net_1=rmse
PCC_Kinetics_Sub_Net_1=p

gc.collect()
gc.collect()

### Kinetics-Sub-DLR-Net-1 ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Kinetics_Sub_Net_1_pcc(inputs_1D_N,inputs_2D_N)



model_2= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])


model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()


model_2.save(path+'model_Kinetics_Sub_1_PCC.h5')


gc.collect()

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_4] = model_2.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))

y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6




###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_4]= model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))


y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100

print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Kinetics_Sub_DLR_Net_1=rmse
PCC_Kinetics_Sub_DLR_Net_1=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

### Kinetics-Sub-JL-Net-1 ###

  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Kinetics_Sub_Net_1(inputs_1D_N,inputs_2D_N)



model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])


model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()


model_3.save(path+'model_Kinetics_Sub_1_JL.h5')


#### All 16 angles prediction  ####


[yhat_1,yhat_2,yhat_4]= model_3.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)


RMSE_Kinetics_Sub_JL_Net_1=rmse
PCC_Kinetics_Sub_JL_Net_1=p

ablation_19=np.hstack([RMSE_Kinetics_Sub_Net_1,PCC_Kinetics_Sub_Net_1])
ablation_20=np.hstack([RMSE_Kinetics_Sub_DLR_Net_1,PCC_Kinetics_Sub_DLR_Net_1])
ablation_21=np.hstack([RMSE_Kinetics_Sub_JL_Net_1,PCC_Kinetics_Sub_JL_Net_1])

Kinetics_Sub_Net_1_result=np.vstack([ablation_19,ablation_20,ablation_21])


####################################################################################################################################################################################################
####################################################################################################################################################################################################

##########################################################################################################################################################################################################################
##########################################################################################################################################################################################################################


""# Kinetics-Sub-Net-2,  Kinetics-Sub-DLR-Net-2,  Kinetics-Sub-JL-Net-2"

### Kinetics-Sub-Net-2 ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Kinetics_Sub_Net_2(inputs_1D_N,inputs_2D_N)

model_1= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])


model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()

gc.collect()


model_1.save(path+'model_Kinetics_Sub_2.h5')

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_4] = model_1.predict([test_X_1D,test_X_2D])





s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100




print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Kinetics_Sub_Net_2=rmse
PCC_Kinetics_Sub_Net_2=p

gc.collect()
gc.collect()

### Kinetics-Sub-DLR-Net-1 ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Kinetics_Sub_Net_2_pcc(inputs_1D_N,inputs_2D_N)



model_2= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])


model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()


model_2.save(path+'model_Kinetics_Sub_2_PCC.h5')


gc.collect()

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_4] = model_2.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))

y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6




###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_4]= model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))


y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100

print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Kinetics_Sub_DLR_Net_2=rmse
PCC_Kinetics_Sub_DLR_Net_2=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

### Kinetics-Sub-JL-Net-2 ###

  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Kinetics_Sub_Net_2(inputs_1D_N,inputs_2D_N)



model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])


model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()


model_3.save(path+'model_Kinetics_Sub_2_JL.h5')


#### All 16 angles prediction  ####


[yhat_1,yhat_2,yhat_4]= model_3.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)


RMSE_Kinetics_Sub_JL_Net_2=rmse
PCC_Kinetics_Sub_JL_Net_2=p

ablation_22=np.hstack([RMSE_Kinetics_Sub_Net_2,PCC_Kinetics_Sub_Net_2])
ablation_23=np.hstack([RMSE_Kinetics_Sub_DLR_Net_2,PCC_Kinetics_Sub_DLR_Net_2])
ablation_24=np.hstack([RMSE_Kinetics_Sub_JL_Net_2,PCC_Kinetics_Sub_JL_Net_2])

Kinetics_Sub_Net_2_result=np.vstack([ablation_22,ablation_23,ablation_24])


####################################################################################################################################################################################################
####################################################################################################################################################################################################

##########################################################################################################################################################################################################################
##########################################################################################################################################################################################################################


""# Kinetics-Sub-Net-3,  Kinetics-Sub-DLR-Net-3,  Kinetics-Sub-JL-Net-3"

### Kinetics-Sub-Net-3 ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Kinetics_Sub_Net_3(inputs_1D_N,inputs_2D_N)

model_1= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])


model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()

gc.collect()


model_1.save(path+'model_Kinetics_Sub_3.h5')

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_4] = model_1.predict([test_X_1D,test_X_2D])





s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100




print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Kinetics_Sub_Net_3=rmse
PCC_Kinetics_Sub_Net_3=p

gc.collect()
gc.collect()

### Kinetics-Sub-DLR-Net-3 ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Kinetics_Sub_Net_3_pcc(inputs_1D_N,inputs_2D_N)



model_2= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])


model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()


model_2.save(path+'model_Kinetics_Sub_3_PCC.h5')


gc.collect()

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_4] = model_2.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))

y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6




###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_4]= model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))


y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]


y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100

print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)

RMSE_Kinetics_Sub_DLR_Net_3=rmse
PCC_Kinetics_Sub_DLR_Net_3=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

### Kinetics-Sub-JL-Net-2 ###

  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Kinetics_Sub_Net_3(inputs_1D_N,inputs_2D_N)


model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])


model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()


model_3.save(path+'model_Kinetics_Sub_3_JL.h5')


#### All 16 angles prediction  ####


[yhat_1,yhat_2,yhat_4]= model_3.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)


RMSE_Kinetics_Sub_JL_Net_3=rmse
PCC_Kinetics_Sub_JL_Net_3=p

ablation_25=np.hstack([RMSE_Kinetics_Sub_Net_3,PCC_Kinetics_Sub_Net_3])
ablation_26=np.hstack([RMSE_Kinetics_Sub_DLR_Net_3,PCC_Kinetics_Sub_DLR_Net_3])
ablation_27=np.hstack([RMSE_Kinetics_Sub_JL_Net_3,PCC_Kinetics_Sub_JL_Net_3])

Kinetics_Sub_Net_3_result=np.vstack([ablation_25,ablation_26,ablation_27])


####################################################################################################################################################################################################
####################################################################################################################################################################################################




"""# Kinetics-Net, Kinetics-DLR-NET, Kinetics-JL-Net"""

### Kinetics-Net ###

w1=100
  
inputs_1D = tf.keras.layers.Input( shape=(w1,18) )
inputs_2D = tf.keras.layers.Input( shape=(w1,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_Net(inputs_1D_N,inputs_2D_N)
# output=Gait_GRU_Net(inputs_1D_N,inputs_2D_N)



# model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

# model_2.compile(loss='mean_squared_error', optimizer='Adam')

model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()

model_1.save(path+'model_Kinetics.h5')


gc.collect()


[yhat_1,yhat_2,yhat_3,yhat_4] = model_1.predict([test_X_1D,test_X_2D])


s=test_y.shape[0]*100
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))





y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)

Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
 
#        ### Saving correlation and RMSE ###

# aa=[x for x in range (2000)]
# aa=[y for y in range(2000)]

# aa=list(range(1,2000))
plt.plot(y_test_2[10000:12000], marker='.', label="Actual")
plt.plot(y_2[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_flexion.png')
plt.show()
                 

plt.plot(y_test_4[10000:12000], marker='.', label="Actual")
plt.plot(y_4[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_abduction.png')
plt.show()
 

plt.plot(y_test_6[10000:12000], marker='.', label="Actual")
plt.plot( y_6[10000:12000], 'r', label="Predicted")
plt.ylabel('value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_rotation.png')
 
plt.show()

gc.collect()

RMSE_Kinetics=rmse
PCC_Kinetics=p

### Kinetics-NET ###

  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_Net_pcc(inputs_1D_N,inputs_2D_N)
# output=Gait_GRU_Net(inputs_1D_N,inputs_2D_N)



# model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

# model_2.compile(loss='mean_squared_error', optimizer='Adam')


model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


# history=model_2.fit([train_X_1D,train_X_2D], train_y_5,, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D],\
                                                                                                        [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

model_2.save(path+'model_Kinetics_PCC.h5')


gc.collect()


[yhat_1,yhat_2,yhat_3,yhat_4] = model_2.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]

y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
 
#        ### Saving correlation and RMSE ###

# aa=[x for x in range (2000)]
# aa=[y for y in range(2000)]

# aa=list(range(1,2000))
plt.plot(y_test_3[0000:2000], marker='.', label="Actual")
plt.plot(y_3[0000:2000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_flexion.png')
plt.show()
                 

plt.plot(y_test_4[0000:2000], marker='.', label="Actual")
plt.plot(y_4[0000:2000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_abduction.png')
plt.show()
 

plt.plot(y_test_6[0000:2000], marker='.', label="Actual")
plt.plot( y_6[0000:2000], 'r', label="Predicted")
plt.ylabel('value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_rotation.png')
 
plt.show()

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)


[yhat_1,yhat_2,yhat_3,yhat_4]= model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
 
#        ### Saving correlation and RMSE ###

# aa=[x for x in range (2000)]
# aa=[y for y in range(2000)]

# aa=list(range(1,2000))
plt.plot(y_test_2[10000:12000], marker='.', label="Actual")
plt.plot(y_2[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_flexion.png')
plt.show()
                 

plt.plot(y_test_4[10000:12000], marker='.', label="Actual")
plt.plot(y_4[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_abduction.png')
plt.show()
 

plt.plot(y_test_6[10000:12000], marker='.', label="Actual")
plt.plot( y_6[10000:12000], 'r', label="Predicted")
plt.ylabel('value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_rotation.png')
 
plt.show()

RMSE_Kinetics_DLR=rmse
PCC_Kinetics_DLR=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()

gc.collect()

### Kinetics-NET ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_Net(inputs_1D_N,inputs_2D_N)
# output=Gait_GRU_Net(inputs_1D_N,inputs_2D_N)



# model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

# model_2.compile(loss='mean_squared_error', optimizer='Adam')

model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()


model_3.save(path+'model_Kinetics_JL.h5')



gc.collect()

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_3,yhat_4] = model_3.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100

print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
 

# aa=list(range(1,2000))
plt.plot(y_test_2[10000:12000], marker='.', label="Actual")
plt.plot(y_2[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_flexion.png')
plt.show()
                 

plt.plot(y_test_4[10000:12000], marker='.', label="Actual")
plt.plot(y_4[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_abduction.png')
plt.show()
 

plt.plot(y_test_6[10000:12000], marker='.', label="Actual")
plt.plot( y_6[10000:12000], 'r', label="Predicted")
plt.ylabel('value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_rotation.png')
 
plt.show()

RMSE_Kinetics_JL=rmse
PCC_Kinetics_JL=p

ablation_28=np.hstack([RMSE_Kinetics,PCC_Kinetics])
ablation_29=np.hstack([RMSE_Kinetics_DLR,PCC_Kinetics_DLR])
ablation_30=np.hstack([RMSE_Kinetics_JL,PCC_Kinetics_JL])

Kinetics_result=np.vstack([ablation_28,ablation_29,ablation_30])



######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################

####################################################################################################################################################################################################
####################################################################################################################################################################################################




"""# Kinetics-FM-Net, Kinetics-FM-DLR-NET, Kinetics-FM-JL-Net"""

### Kinetics-Net ###

w1=100
  
inputs_1D = tf.keras.layers.Input( shape=(w1,18) )
inputs_2D = tf.keras.layers.Input( shape=(w1,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)
# output=Gait_GRU_Net(inputs_1D_N,inputs_2D_N)



# model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

# model_2.compile(loss='mean_squared_error', optimizer='Adam')

model_1.compile(loss=correlation_coefficient_loss_1, optimizer='Adam', metrics=[correlation_coefficient_loss_1])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_1.summary()

model_1.save(path+'model_Kinetics_FM.h5')


gc.collect()


[yhat_1,yhat_2,yhat_3,yhat_4] = model_1.predict([test_X_1D,test_X_2D])


s=test_y.shape[0]*100
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))





y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)

Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
 
#        ### Saving correlation and RMSE ###

# aa=[x for x in range (2000)]
# aa=[y for y in range(2000)]

# aa=list(range(1,2000))
plt.plot(y_test_2[10000:12000], marker='.', label="Actual")
plt.plot(y_2[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_flexion.png')
plt.show()
                 

plt.plot(y_test_4[10000:12000], marker='.', label="Actual")
plt.plot(y_4[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_abduction.png')
plt.show()
 

plt.plot(y_test_6[10000:12000], marker='.', label="Actual")
plt.plot( y_6[10000:12000], 'r', label="Predicted")
plt.ylabel('value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_rotation.png')
 
plt.show()

gc.collect()

RMSE_Kinetics_FM=rmse
PCC_Kinetics_FM=p

### Kinetics-NET ###

  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_FM_Net_pcc(inputs_1D_N,inputs_2D_N)
# output=Gait_GRU_Net(inputs_1D_N,inputs_2D_N)



# model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

# model_2.compile(loss='mean_squared_error', optimizer='Adam')


model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


# history=model_2.fit([train_X_1D,train_X_2D], train_y_5,, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D],\
                                                                                                        [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

model_2.save(path+'model_Kinetics_FM_PCC.h5')


gc.collect()


[yhat_1,yhat_2,yhat_3,yhat_4] = model_2.predict([test_X_1D,test_X_2D])

s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]

y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]


cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Y_1=y_1
Y_2=y_2
Y_3=y_3
Y_4=y_4
Y_5=y_5
Y_6=y_6



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100


print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)


p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
 
#        ### Saving correlation and RMSE ###

# aa=[x for x in range (2000)]
# aa=[y for y in range(2000)]

# aa=list(range(1,2000))
plt.plot(y_test_3[0000:2000], marker='.', label="Actual")
plt.plot(y_3[0000:2000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_flexion.png')
plt.show()
                 

plt.plot(y_test_4[0000:2000], marker='.', label="Actual")
plt.plot(y_4[0000:2000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_abduction.png')
plt.show()
 

plt.plot(y_test_6[0000:2000], marker='.', label="Actual")
plt.plot( y_6[0000:2000], 'r', label="Predicted")
plt.ylabel('value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_rotation.png')
 
plt.show()

def estimate_coef(x, y):
    # number of observations/points
    n = np.size(x)
  
    # mean of x and y vector
    m_x = np.mean(x)
    m_y = np.mean(y)
  
    # calculating cross-deviation and deviation about x
    SS_xy = np.sum(y*x) - n*m_y*m_x
    SS_xx = np.sum(x*x) - n*m_x*m_x
  
    # calculating regression coefficients
    b_1 = SS_xy / SS_xx
    b_0 = m_y - b_1*m_x
  
    return (b_0, b_1)

a_1,b_1=estimate_coef(Y_1,Z_1)
a_2,b_2=estimate_coef(Y_2,Z_2)
a_3,b_3=estimate_coef(Y_3,Z_3)
a_4,b_4=estimate_coef(Y_4,Z_4)
a_5,b_5=estimate_coef(Y_5,Z_5)
a_6,b_6=estimate_coef(Y_6,Z_6)


[yhat_1,yhat_2,yhat_3,yhat_4]= model_2.predict([test_X_1D,test_X_2D])



s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]




y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]



cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)



y_1=y_1*b_1+a_1
y_2=y_2*b_2+a_2
y_3=y_3*b_3+a_3
y_4=y_4*b_4+a_4
y_5=y_5*b_5+a_5
y_6=y_6*b_1+a_1


y_bag_1=np.vstack((y_1,y_2,y_3,y_4,y_5,y_6))
y_bag_1=np.transpose(y_bag_1)



###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100



print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)




p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]


print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)



            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
 
#        ### Saving correlation and RMSE ###

# aa=[x for x in range (2000)]
# aa=[y for y in range(2000)]

# aa=list(range(1,2000))
plt.plot(y_test_2[10000:12000], marker='.', label="Actual")
plt.plot(y_2[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_flexion.png')
plt.show()
                 

plt.plot(y_test_4[10000:12000], marker='.', label="Actual")
plt.plot(y_4[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_abduction.png')
plt.show()
 

plt.plot(y_test_6[10000:12000], marker='.', label="Actual")
plt.plot( y_6[10000:12000], 'r', label="Predicted")
plt.ylabel('value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_rotation.png')
 
plt.show()

RMSE_Kinetics_FM_DLR=rmse
PCC_Kinetics_FM_DLR=p

gc.collect()
gc.collect()
gc.collect()
gc.collect()


### Kinetics-NET ###


  
inputs_1D = tf.keras.layers.Input( shape=(w,18) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)
# output=Gait_GRU_Net(inputs_1D_N,inputs_2D_N)



# model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

# model_2.compile(loss='mean_squared_error', optimizer='Adam')

model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


# history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=25, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)
history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_3.summary()


model_3.save(path+'model_Kinetics_FM_JL.h5')



gc.collect()

#### All 16 angles prediction  ####

[yhat_1,yhat_2,yhat_3,yhat_4] = model_3.predict([test_X_1D,test_X_2D])


s=(a2+1)*w
 
test_o=test_y.reshape((s,6))
yhat=yhat_4.reshape((s,6))




y_1_no=yhat[:,0]
y_2_no=yhat[:,1]
y_3_no=yhat[:,2]
y_4_no=yhat[:,3]
y_5_no=yhat[:,4]
y_6_no=yhat[:,5]



y_test_1=test_o[:,0]
y_test_2=test_o[:,1]
y_test_3=test_o[:,2]
y_test_4=test_o[:,3]
y_test_5=test_o[:,4]
y_test_6=test_o[:,5]




cutoff=6
fs=200
order=4

nyq = 0.5 * fs
## filtering data ##
def butter_lowpass_filter(data, cutoff, fs, order):
    normal_cutoff = cutoff / nyq
    # Get the filter coefficients 
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    y = filtfilt(b, a, data)
    return y



y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)
y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)
y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)
y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)
y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)
y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)


Z_1=y_1
Z_2=y_2
Z_3=y_3
Z_4=y_4
Z_5=y_5
Z_6=y_6

###calculate RMSE

rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100
rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100
rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100

print(rmse_1)
print(rmse_2)
print(rmse_3)
print(rmse_4)
print(rmse_5)
print(rmse_6)



p_1=np.corrcoef(y_1, y_test_1)[0, 1]
p_2=np.corrcoef(y_2, y_test_2)[0, 1]
p_3=np.corrcoef(y_3, y_test_3)[0, 1]
p_4=np.corrcoef(y_4, y_test_4)[0, 1]
p_5=np.corrcoef(y_5, y_test_5)[0, 1]
p_6=np.corrcoef(y_6, y_test_6)[0, 1]

print("\n") 
print(p_1)
print(p_2)
print(p_3)
print(p_4)
print(p_5)
print(p_6)


            ### Correlation ###
p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])




    #### Mean and standard deviation ####

rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])

    #### Mean and standard deviation ####
m=statistics.mean(rmse)
SD=statistics.stdev(rmse)
print('Mean: %.3f' % m,'+/- %.3f' %SD)
 
m_c=statistics.mean(p)
SD_c=statistics.stdev(p)
print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
 

# aa=list(range(1,2000))
plt.plot(y_test_2[10000:12000], marker='.', label="Actual")
plt.plot(y_2[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_flexion.png')
plt.show()
                 

plt.plot(y_test_4[10000:12000], marker='.', label="Actual")
plt.plot(y_4[10000:12000], 'r', label="Predicted")
plt.ylabel('Value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_abduction.png')
plt.show()
 

plt.plot(y_test_6[10000:12000], marker='.', label="Actual")
plt.plot( y_6[10000:12000], 'r', label="Predicted")
plt.ylabel('value', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=10)
#plt.savefig('Hip_rotation.png')
 
plt.show()

RMSE_Kinetics_FM_JL=rmse
PCC_Kinetics_FM_JL=p

ablation_31=np.hstack([RMSE_Kinetics_FM,PCC_Kinetics_FM])
ablation_32=np.hstack([RMSE_Kinetics_FM_DLR,PCC_Kinetics_FM_DLR])
ablation_33=np.hstack([RMSE_Kinetics_FM_JL,PCC_Kinetics_FM_JL])

Kinetics_FM_result=np.vstack([ablation_31,ablation_32,ablation_33])



######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################



All_result=np.vstack([GRU_result,Conv2D_result,Conv1D_result,GRU_Conv2D_result,GRU_Conv1D_result,Conv2D_Conv1D_result,Kinetics_Sub_Net_1_result,Kinetics_Sub_Net_2_result,Kinetics_Sub_Net_3_result,Kinetics_result,Kinetics_FM_result])

from numpy import savetxt

savetxt(path+'All_results.csv', All_result, delimiter=',')




######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################




